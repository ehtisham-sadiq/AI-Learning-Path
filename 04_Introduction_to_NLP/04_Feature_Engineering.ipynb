{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 align=\"center\"><b>Feature Engineering</b></h1>\n",
    "\n",
    "<h3><div align=\"right\">Ehtisham Sadiq</div></h3>    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img align=\"center\" width=\"900\"  src=\"images/phase3.png\"  > "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning Agenda of this Notebook\n",
    "\n",
    "- **Overview of Feature Engineering and Vectorization**\n",
    "  \n",
    "- **Frequency or Statistical Based Approaches**\n",
    "  - Label Encoding\n",
    "  - One-Hot Encoding\n",
    "  - Bag of Words\n",
    "  - Bag of n-grams\n",
    "  - TF-IDF\n",
    "\n",
    "- **Prediction-Based Approaches (Embeddings)**\n",
    "  - Word2Vec — From Google\n",
    "  - FastText — From Facebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style='background :lightgreen' > Overview of Feature Engineering and Text Vectorization </span>\n",
    "\n",
    "## a. What do you mean by `Feature` in Machine Learning?\n",
    "\n",
    "<!-- <img src=\"images/pca5.png\" style=\"align:center\" height=500px width=900px> -->\n",
    "<img src=\"images/pca5.png\" style=\"display: block; margin: 0 auto;\" height=\"400px\" width=\"900px\">\n",
    "\n",
    "<br/>\n",
    "\n",
    "> **Feature engineering involves identifying and transforming attributes `(features)` from raw data into a format suitable for machine learning models. Think of it as crafting the input that powers AI systems.**\n",
    "\n",
    "Imagine predicting house prices. `Features` could include:\n",
    "- The city it’s in.\n",
    "- The size of the house.\n",
    "- The number of bedrooms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## b. What do you mean by `Feature` in Natural Language Processing?\n",
    "\n",
    "# <h3 align=\"center\"><div class=\"alert alert-success\" style=\"margin: 20px\">Andrew Ng Youtube Lectures are amazing.</h3>\n",
    "\n",
    "Similarly, in `NLP`:\n",
    "- A `feature` can be a single word, phrase, sentence, or even an entire document.\n",
    "- Our job is to represent these features as numbers. Why? Because machine learning models can only understand numbers, not plain text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## c. What is Feature Engineering or Text Vectorization?\n",
    "\n",
    "> **It’s the process of converting text into vectors (arrays of numbers) that represent the meaning or frequency of the text.**\n",
    "\n",
    "<img src=\"images/text-rep3.png\" style=\"display: block; margin: 0 auto;\" height=\"500px\" width=\"900px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Feature Extraction from Text (Text Representation or Text Vectorization)\n",
    "\n",
    "The process of converting text data into vectors of real numbers is called **Feature Extraction from Text** or **Text Representation/Text Vectorization**. The goal of this process is to transform textual data into numerical form, ensuring that these numbers can capture the semantic or contextual meaning of the words.\n",
    "\n",
    "In simple terms:\n",
    "- **Text Vectorization** converts words or phrases into a format that machines can process, such as vectors of numbers.\n",
    "- These numbers represent the meaning or attributes of the text in a mathematical form.\n",
    "\n",
    "\n",
    "**Example**\n",
    "The example demonstrates how different textual entities can be represented numerically based on their attributes or properties:\n",
    "1. Each vector contains numerical representations of features such as:\n",
    "   - **Person** (1 or 0, denoting if it is a person)\n",
    "   - **Healthy/Fit** (a continuous value representing fitness level)\n",
    "   - **Location** (binary representation indicating presence as a location)\n",
    "   - **Has two eyes** (binary attribute indicating presence of this feature)\n",
    "   - **Has Government** (binary attribute for governmental entities)\n",
    "\n",
    "2. The vectors in the image:\n",
    "   - Blue vector `[1, 0.8, 0, 1, 0]`: Represents a person who is healthy, not a location, has two eyes, and does not represent a government.\n",
    "   - Orange vector `[1, 0.9, 0, 1, 0]`: Another individual with similar attributes but a higher \"Healthy/Fit\" value.\n",
    "   - Green vector `[0, 0.6, 1, 0, 1]`: Represents a location entity with medium fitness value, does not have two eyes, but represents a government.\n",
    "\n",
    "This approach ensures that text entities are numerically encoded for further machine learning or natural language processing tasks.\n",
    "\n",
    "\n",
    "**Importance**\n",
    "- **Why Feature Extraction from Text?**\n",
    "   - Converts unstructured textual data into structured numerical data.\n",
    "   - Enables the use of machine learning models for text classification, clustering, and other NLP tasks.\n",
    "   - Represents textual data in a way that preserves semantic meaning and relationships."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## d. Why to do Text Vectorization?\n",
    "\n",
    "\n",
    "<img src=\"images/text-rep2.png\" style=\"display: block; margin: 0 auto;\" height=\"900px\" width=\"1300px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Convert Text to Numerical Representations**  \n",
    "   - Text vectorization transforms textual features like words, phrases, or entire documents into numerical vectors, enabling machine learning algorithms to process and analyze the data.  \n",
    "   - Example: Words like \"football\" and \"basketball\" are represented as numeric vectors `[3, 1]`.\n",
    "\n",
    "2. **Enable Mathematical and Statistical Operations**  \n",
    "   - Text vectors allow for mathematical operations such as addition, subtraction, and similarity measurement.  \n",
    "   - This enables algorithms to assess relationships between words, phrases, or documents effectively.\n",
    "\n",
    "3. **Facilitate Document Similarity Analysis**  \n",
    "   - By representing documents as vectors, their similarity can be calculated using methods like **cosine similarity**.  \n",
    "   - Example: Two documents with angles close to 0 degrees are more similar, while those with larger angles (e.g., 90 degrees) are dissimilar.\n",
    "\n",
    "4. **Handle High-Dimensional Text Features**  \n",
    "   - Although visualizing low-dimensional vectors is simple, real-world text data involves thousands of features.  \n",
    "   - Text vectorization techniques like cosine similarity enable efficient handling and comparison of such high-dimensional data.\n",
    "\n",
    "\n",
    "#### **Difference Between Cosine Similarity and Cosine Distance**\n",
    "\n",
    "##### **Cosine Similarity**\n",
    "- **Definition**: Measures the cosine of the angle between two vectors in a multidimensional space.\n",
    "- **Range**: Values range from 0 to 1:\n",
    "  - `1`: Vectors are identical (angle = 0 degrees).\n",
    "  - `0`: Vectors are orthogonal or completely different (angle = 90 degrees).\n",
    "\n",
    "##### **Cosine Distance**\n",
    "- **Definition**: Measures the dissimilarity between two vectors by subtracting the cosine similarity from 1.\n",
    "- **Range**: Values range from 0 to 1:\n",
    "  - `0`: Vectors are identical.\n",
    "  - `1`: Vectors are completely different.\n",
    "\n",
    "#### **When to Use Each**\n",
    "\n",
    "1. **Cosine Similarity**\n",
    "   - **Use Case**: When you want to **compare similarity** between text data or documents.  \n",
    "     Examples:\n",
    "       - Ranking search results based on query-document similarity.\n",
    "       - Measuring how similar two product reviews or news articles are.\n",
    "   - **Scenario**: Useful in recommendation systems, document clustering, and NLP tasks where identifying closeness in meaning is the goal.\n",
    "\n",
    "2. **Cosine Distance**\n",
    "   - **Use Case**: When you want to focus on **dissimilarity** or differences between vectors.  \n",
    "     Examples:\n",
    "       - Finding outliers in datasets.\n",
    "       - Evaluating how distinct one document is from another.\n",
    "   - **Scenario**: Useful in anomaly detection, classification tasks, or when penalizing differences is critical."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## e. How to do Text Vectorization?\n",
    "\n",
    "\n",
    "<img src=\"images/vec-techniques.jpg\" style=\"display: block; margin: 0 auto;\" height=\"600px\" width=\"1000px\">\n",
    "\n",
    "\n",
    "\n",
    "##### **1. Frequency or Statistical-Based Approaches**\n",
    "These methods rely on counting or transforming the frequency of words in the text.\n",
    "\n",
    "- **Label Encoding**: Assigns unique integers to each word.\n",
    "- **One-Hot Encoding**: Creates binary vectors with one \"hot\" (1) value for the presence of a word and 0 elsewhere.\n",
    "- **Bag of Words**: Represents text by the frequency of words, ignoring grammar and word order.\n",
    "- **Bag of N-Grams**: Considers combinations of words (n-grams) to capture more context.\n",
    "- **TF-IDF**: Combines term frequency (TF) and inverse document frequency (IDF) to weight words based on their importance in the document.\n",
    "\n",
    "\n",
    "\n",
    "##### **2. Prediction-Based Approaches (Embeddings)**\n",
    "These approaches rely on neural networks to generate dense vector representations of words.\n",
    "\n",
    "- **Word2Vec**: Developed by Google, uses shallow neural networks to create word embeddings.\n",
    "- **FastText**: Developed by Facebook, extends Word2Vec by capturing subword information, making embeddings useful for rare words.\n",
    "\n",
    "\n",
    "\n",
    "##### **Process Workflow for Text Vectorization**\n",
    "The process of text vectorization involves the following steps:\n",
    "\n",
    "1. **Corpus Preparation**:\n",
    "   - Start with raw textual data or a document collection.\n",
    "\n",
    "2. **Preprocessing**:\n",
    "   - **Splitting**: Split text into smaller components like sentences or words.\n",
    "   - **Noise Removal**: Remove unnecessary characters, symbols, or stopwords.\n",
    "   - **Normalization**: Standardize text (e.g., convert to lowercase).\n",
    "\n",
    "3. **Tokenization**:\n",
    "   - Split the text into individual words or tokens.\n",
    "\n",
    "4. **Token-ID Mapping**:\n",
    "   - Map tokens to their respective IDs using:\n",
    "     - Vocabulary lookup for known words.\n",
    "     - Feature hashing for unknown or rare words.\n",
    "\n",
    "5. **Vectorization Outputs**:\n",
    "   - **One-Hot Encoding**\n",
    "   - **Count Vectors (+ TF-IDF)**\n",
    "   - **Word Embeddings**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example of Token-ID Mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "\n",
    "# Vocabulary lookup for known words\n",
    "vocabulary = {\n",
    "    \"person\": 1,\n",
    "    \"healthy\": 2,\n",
    "    \"fit\": 3,\n",
    "    \"location\": 4,\n",
    "    \"government\": 5\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input text\n",
    "text = \"person healthy fit unknown\"\n",
    "\n",
    "# Tokenize the text\n",
    "tokens = text.split()\n",
    "\n",
    "# Token-ID Mapping using vocabulary lookup\n",
    "token_ids = [vocabulary.get(token, None) for token in tokens]\n",
    "print(\"Token IDs using vocabulary lookup:\")\n",
    "print(token_ids)  # Known tokens will have IDs, unknown will be None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature hashing for unknown or rare words\n",
    "vectorizer = HashingVectorizer(n_features=10)  # 10 hash buckets\n",
    "hashed_features = vectorizer.transform([text])\n",
    "\n",
    "print(\"\\nFeature hashing output:\")\n",
    "print(hashed_features.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Label Encoding\n",
    "\n",
    "#### Definitions:\n",
    "- **Document**: A single text data point (e.g., a tweet, a YouTube comment, or a product review).\n",
    "- **Corpus**: A collection of all documents in the dataset.\n",
    "- **Feature**: Every unique word in the corpus.\n",
    "\n",
    "#### Example:\n",
    "- Consider the following corpus that consist of three documents:\n",
    "\n",
    "    ```boldtext\n",
    "    doc1 = [\"Ali help his students\"]\n",
    "    doc2 = [\"Ali assist his students\"]\n",
    "    doc3 = [\"Ali lectures are great\"]\n",
    "    ```\n",
    "**Vocabulary:**\n",
    "- To build a vocabulary, assign a unique number to each word in the corpus:\n",
    "\n",
    "    ```boldtext\n",
    "    vocab = {1: 'students', 2: 'assist', 3: 'his', 4: 'help', 5: 'Ali', 6: 'lectures', 7: 'great', 8: 'are'}\n",
    "    ```\n",
    "\n",
    "**Vectorized Documents:**\n",
    "- Each word in the document is replaced by its corresponding number from the vocabulary:\n",
    "\n",
    "    ```boldtext\n",
    "    doc1 = [5, 4, 3, 1]  # \"Ali help his students\"\n",
    "    doc2 = [5, 2, 3, 1]  # \"Ali assist his students\"\n",
    "    doc3 = [5, 6, 8, 7]  # \"Ali lectures are great\"\n",
    "    ```\n",
    "\n",
    "- **Limitations of Label Encoding**\n",
    "    - **Size is not Fixed**:\n",
    "    - Example: Consider a new document: `Ali students are great help`. \n",
    "        - When vectorized, this document will have a size of 5 instead of 4 (as expected by the machine learning algorithm).\n",
    "        - Machine learning algorithms often expect fixed-size input, making such cases unprocessable.\n",
    "  \n",
    "    - **Out of Vocabulary Problem (OOV)**:\n",
    "    - Example: Consider a new document: `Ali YouTube lectures are great`. \n",
    "        - The word `YouTube` is not in the vocabulary, so it cannot be encoded.\n",
    "        - This is known as the Out of Vocabulary (OOV) problem.\n",
    "\n",
    "    - **Cannot Capture Semantics/Meanings**:\n",
    "    - The words `help` and `assist` are almost similar in meaning, but they are represented completely differently in label encoding. \n",
    "        - As a result, semantic relationships are lost.\n",
    "\n",
    "    > **Conclusion:** Due to these limitations, **Label Encoding** is not suitable for transforming text data into numbers for most Natural Language Processing (NLP) applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python Implementation\n",
    "corpus = [\n",
    "    \"Ali help his students\",\n",
    "    \"Ali assist his students\",\n",
    "    \"Ali lectures are great\"\n",
    "]\n",
    "\n",
    "# Build Vocabulary\n",
    "vocab = {'students': 1, 'assist': 2, 'his': 3, 'help': 4, 'Ali': 5, 'lectures': 6, 'great': 7, 'are': 8}\n",
    "\n",
    "# Function to encode documents\n",
    "def encode_document(doc, vocab):\n",
    "    return [vocab[word] for word in doc.split()]\n",
    "\n",
    "# Encode each document\n",
    "doc1_encoded = encode_document(\"Ali help his students\", vocab)\n",
    "doc2_encoded = encode_document(\"Ali assist his students\", vocab)\n",
    "doc3_encoded = encode_document(\"Ali lectures are great\", vocab)\n",
    "# doc4_encoded = encode_document(\"Ali lectures are not great\", vocab)\n",
    "\n",
    "print(\"Encoded Documents:\")\n",
    "print(\"doc1:\", doc1_encoded)\n",
    "print(\"doc2:\", doc2_encoded)\n",
    "print(\"doc3:\", doc3_encoded)\n",
    "# print(\"doc4:\", doc4_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. One-Hot Encoding\n",
    "\n",
    "- Consider the following corpus that consists of three documents and the corresponding vocabulary of the corpus:\n",
    "\n",
    "    ```boldtext\n",
    "    doc1 = [\"Ali help his students\"]\n",
    "    doc2 = [\"Ali assist his students\"]\n",
    "    doc3 = [\"Ali lectures are great\"]\n",
    "    ```\n",
    "\n",
    "```boldtext    \n",
    "vocab = {'students', 'assist', 'his', 'help', 'Ali', 'lectures', 'great', 'are'}\n",
    "```\n",
    "\n",
    "- Encode every word as a binary vector of the same size as the number of words in the vocabulary, with only one location having a value of 1, and that is under the word.\n",
    "- The columns in the following matrix are the words in the vocabulary, while rows are the words with their vector representation:\n",
    "\n",
    "| Word      | students | assist | his | help | Ali | lectures | great | are |\n",
    "|-----------|----------|--------|-----|------|-----|----------|-------|-----|\n",
    "| Ali       | 0        | 0      | 0   | 0    | 1   | 0        | 0     | 0   |\n",
    "| help      | 0        | 0      | 0   | 1    | 0   | 0        | 0     | 0   |\n",
    "| his       | 0        | 0      | 1   | 0    | 0   | 0        | 0     | 0   |\n",
    "| students  | 1        | 0      | 0   | 0    | 0   | 0        | 0     | 0   |\n",
    "| assist    | 0        | 1      | 0   | 0    | 0   | 0        | 0     | 0   |\n",
    "| great     | 0        | 0      | 0   | 0    | 0   | 0        | 1     | 0   |\n",
    "| are       | 0        | 0      | 0   | 0    | 0   | 0        | 0     | 1   |\n",
    "| lectures  | 0        | 0      | 0   | 0    | 0   | 1        | 0     | 0   |\n",
    "\n",
    "- In One-Hot Encoding, you represent every word of a document as a vector of size ( v ), where ( v ) is the size of the vocabulary.\n",
    "- The vectorization of the three documents using one-hot encoding is shown below:\n",
    "\n",
    "    ```boldtext\n",
    "    doc1 = [\"Ali help his students\"] = [[00001000], [00010000], [00100000], [10000000]]\n",
    "    doc2 = [\"Ali assist his students\"] = [[00001000], [01000000], [00100000], [10000000]]\n",
    "    doc3 = [\"Ali lectures are great\"] = [[00001000], [00000001], [00000010], [00000100]]\n",
    "    ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Limitations**:\n",
    "- **Size is not Fixed:**\n",
    "    - Consider a document (Ali students are great help) that we need to classify. Once you vectorize this document, the size of the sparse matrix representing this document will be ( (5, 8) ), i.e., 40 numbers.\n",
    "\t- Unfortunately, our machine learning algorithms expect same/fixed size input, and therefore, we will not be able to process it.\n",
    "- **Out of Vocabulary Problem (OOV)**:\n",
    "\t- Consider a document (Ali YouTube lectures are great). It has a new word `YouTube` that is not there in the vocabulary.\n",
    "\t- So we will not be able to encode it. This is called an out of vocabulary (OOV) problem.\n",
    "- **Cannot Capture Semantic Meanings:**\n",
    "\t- The word `help` and `assist` are almost similar in meaning, but these two words have completely different representations in one-hot encoding.\n",
    "- **Sparse Matrix Representation:**\n",
    "\t- This technique is memory-hungry. For example, in the above toy example, each document is represented by a sparse matrix of size ( (4, 8) ), i.e., 32 numbers, out of which 28 are zero.\n",
    "\n",
    "> **Due to these limitations, One-Hot Encoding is not used for transforming text data into numbers in any of the NLP applications.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the vocabulary\n",
    "vocab = ['students', 'assist', 'his', 'help', 'Ali', 'lectures', 'great', 'are']\n",
    "\n",
    "# Define the documents\n",
    "documents = [\n",
    "    \"Ali help his students\",\n",
    "    \"Ali assist his students\",\n",
    "    \"Ali lectures are great\"\n",
    "]\n",
    "\n",
    "# Create a one-hot encoding matrix\n",
    "one_hot_matrix = {word: [1 if word == token else 0 for token in vocab] for word in vocab}\n",
    "\n",
    "# Display the one-hot encoded table\n",
    "print(f\"{'Word':<10} {' '.join([f'{token:<8}' for token in vocab])}\")\n",
    "for word in vocab:\n",
    "    print(f\"{word:<10} {' '.join([str(one_hot_matrix[word][i]).ljust(8) for i in range(len(vocab))])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Bag of Words (BoW) Encoding\n",
    "\n",
    "### a. Conceptual Understanding\n",
    "- `Bag of Words (BoW)` is the most basic strategy for converting a text document into numbers, which specifies the presence/count of a word/n-grams in a vocabulary.\n",
    "- The most common NLP application in which we use BoW representation is text classification (e.g., classifying a collection of documents into categories like sports, entertainment, and politics).\n",
    "- Consider the following corpus that consists of three documents consisting of five, ten, and five words respectively and the corresponding vocabulary of the corpus:\n",
    "\n",
    "\t```boldtext\n",
    "\tdoc1 = [\"Ali YouTube lectures are amazing\"]\n",
    "\tdoc2 = [\"I like YouTube lectures and Khurram also like YouTube lectures\"]\n",
    "\tdoc3 = [\"Ali YouTube lectures are great\"]\n",
    "\t```\n",
    "**Vocabulary Mapping**\n",
    "\t```\n",
    "\tvocab = {'also': 0, 'amazing': 1, 'and': 2, 'are': 3, 'ali': 4, 'great': 5, 'khurram': 6, 'lectures': 7, 'like': 8, 'youtube': 9}\n",
    "\t```\n",
    "\n",
    "- Irrespective of the size, each document is converted into a `v-dimensional frequency vector`, where `v` is the size of the vocabulary.\n",
    "- The three documents are represented as `Document-Term Matrix (DTM)`, which is  mathematical matrix that describe the frequency of terms that occur in a collection of documents.\n",
    "\n",
    "**In Theory**\n",
    "| Word     | also | amazing | and | are | ali | great | khurram | lectures | like | youtube |\n",
    "|----------|------|---------|-----|-----|-----|-------|---------|----------|------|---------|\n",
    "| **doc1** | 0    | 1       | 0   | 1   | 1   | 0     | 0       | 1        | 0    | 1       |\n",
    "| **doc2** | 1    | 0       | 1   | 0   | 0   | 0     | 1       | 2        | 2    | 2       |\n",
    "| **doc3** | 0    | 0       | 0   | 1   | 1   | 1     | 0       | 1        | 0    | 1       |\n",
    "\n",
    "\n",
    "**In Practice**\n",
    "| Index | 0 | 1 | 2 | 3 | 4 | 5 | 6 | 7 | 8 | 9 |\n",
    "|-------|---|---|---|---|---|---|---|---|---|---|\n",
    "| **doc1** | 0 | 1 | 0 | 1 | 1 | 0 | 0 | 1 | 0 | 1 |\n",
    "| **doc2** | 1 | 0 | 1 | 0 | 0 | 0 | 1 | 2 | 2 | 2 |\n",
    "| **doc3** | 0 | 0 | 0 | 1 | 1 | 1 | 0 | 1 | 0 | 1 |\n",
    "\n",
    "\n",
    "- `doc1` has a total of 5 words each appearing once, and it is represented as a vector of size 10 (count of vocab), having 5 non-zero values.\n",
    "\n",
    "- `doc2` has a total of 9 words with 6 unique words (excluding single character I). The word “like”, “YouTube”, and “lectures” are coming twice, the rest of the words are appearing once. doc2 is represented as a vector of size 10.\n",
    "\n",
    "- `doc3` has a total of 5 words, each appearing once, and it is represented as a vector of size 10 (count of vocab), having 5 non-zero values.\n",
    "\n",
    "**Advantage:**\n",
    "- `Size is Fixed:` Unlike one-hot encoding, which encodes every word separately, BoW technique encodes every document as a fixed-size vector irrespective of the number of words in it. So this can now be easily fed to a machine learning algorithm.\n",
    "\n",
    "**Limitations:**\n",
    "- `Size Reduced but Still Large:` The vector representation of a document is small in size as compared to one-hot encoding.\n",
    "- `Sparsity Reduced but Still Exists:` A bit better than one-hot encoding; however, vector representation of BoW still has lots and lots of zero values.\n",
    "- `OOV Partially Solved:` In BoW, a word which is not in the vocabulary will be valued as zero.\n",
    "- `Semantic Meaning are Partially Captured:` A bit better than one-hot encoding; however, BoW does not capture the meaning of sentences accurately.\n",
    "- `Ordering of Words is Ignored:` In BoW representation, the ordering of words is not captured.\n",
    "- `Two Very Similar Vectors Convey Completely Different Meanings:` For instance, “Ali YouTube lectures are very good” and “Ali YouTube lectures are not very good” will have similar vectors in BoW but convey opposite meanings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b. Creating Bag of Words using `CountVectorizer`\n",
    "\n",
    "- **Steps to create a BOW representation of a corpus programmatically:**\n",
    "    - **Tokenization**: First, tokenize all the input documents.\n",
    "    - **Vocabulary creation**: Of all the obtained tokenized words, only unique words are selected to create the vocabulary and then sorted by alphabetical order.\n",
    "    - **Vector creation**: Finally, a sparse matrix is created in which each row is a document vector whose length (the columns of the matrix) is equal to the size of the vocabulary. The value of each cell in a row/document is the frequency count of the word under that column.\n",
    "\n",
    "- **Scikit-learn's `CountVectorizer`**: The `CountVectorizer` computes the frequency of occurrence of a word in a document. It converts the corpus of multiple documents (say product reviews) into a Document Term Matrix (a sparse matrix). It also allows you to:\n",
    "    - Control your n-gram size.\n",
    "    - Perform custom preprocessing.\n",
    "    - Perform custom tokenization.\n",
    "    - Eliminate stop words.\n",
    "    - Limit vocabulary size.\n",
    "\n",
    "    ```boldtext\n",
    "    cv = sklearn.feature_extraction.text.CountVectorizer(arg1, arg2, arg3,......, arg16)\n",
    "    ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Corpus of documents\n",
    "corpus = [\n",
    "    \"Ali youTube lectures are amazing\",\n",
    "    \"I like youTube lectures and Khurram also like youTube lectures\",\n",
    "    \"Ali youTube lectures are great\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing necessary libraries\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Create an object of the CountVectorizer class\n",
    "cv = CountVectorizer()\n",
    "\n",
    "# Check the type of the object\n",
    "type(cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow = cv.fit_transform(corpus) # generates vocabulary dictionary and returns a DTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir(cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to print the vocabulary\n",
    "print(cv.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cv.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Note that single characters are not included in the vocabulary.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since the bow is sparse matrix, we need to convert it to a dense matrix by using numpy toarray() or todense() method\n",
    "bow.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Let us understand the sparsity of the matrix.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Total count of values in the BOW matrix\n",
    "total_cells = bow.shape[0] * bow.shape[1]\n",
    "total_cells\n",
    "\n",
    "# Total count of non-zero cells\n",
    "nonzero_cells = bow.nnz\n",
    "nonzero_cells\n",
    "total_cells, nonzero_cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Percentage of non-zero values in the document term matrix\n",
    "percentage = (nonzero_cells / total_cells) * 100\n",
    "percentage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Since this is a toy example, around 47% of the cells contain zero values.\n",
    "- In real-world examples, approximately 99% of the cells contain zero values.\n",
    "- In order to save memory space and speed up algebraic operations, we use the sparse representation of matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Let us save the corpus as Document Term Matrix of BoW Representation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "dmt = pd.DataFrame(bow.toarray(), columns=cv.get_feature_names_out())\n",
    "dmt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dmt2 = pd.DataFrame(bow.toarray())\n",
    "dmt2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c. **Hyper-Parameters of CountVectorizer**\n",
    "\n",
    "- To improve or fine-tune the results on your dataset, you can tweak different hyperparameters of the `CountVectorizer()` method. Some important ones are mentioned below:\n",
    "\n",
    "  ```boldtext\n",
    "  cv = sklearn.feature_extraction.text.CountVectorizer(arg1, arg2, ...)\n",
    "  ```\n",
    "\n",
    "**Where:**\n",
    "  * **vocabulary**: None is the default, created from the input documents. You can pass a Python dictionary where keys are terms and values are indices in the feature matrix.\n",
    "  * **lowercase**: True converts characters of all the documents to lowercase before tokenizing.\n",
    "  * **tokenizer**: None. The default tokenization in `CountVectorizer` removes all special characters, punctuation, and single characters. If this is not the behavior you desire, you can pass a custom tokenizer.\n",
    "  * **stop\\_words**: None. By default, it will not remove any stop words. You can pass a custom list or sklearn's built-in English stop-word list.\n",
    "  * **preprocessor**: None. If passed, a function name that performs customized pre-processing by changing to lower case, removing characters of your choice, including stemming, lemmatization.\n",
    "  * **binary**: False by default shows a frequency count from 0, 1, 2, 3, .... If you are not interested in the frequency of words, rather, just want to know whether the word exists in a document or not, set `binary=True`. It sets all non-zero counts to 1. It is recommended to set this argument to True if you are doing sentiment analysis.\n",
    "  * **max\\_features**: None. If you want to put a limit on the number of features (vocabulary size), then you pass an integer value to this argument. For example, a value of 100 will keep the top 100 most frequent words in the vocabulary and drop the rest.\n",
    "  * **ngram\\_range**: (1, 1). The lower and upper boundary of the range of n-values for different word n-grams or char n-grams to be extracted. All values of n such that `min_n <= n <= max_n` will be used. For example, an ngram\\_range of (1, 1) means only unigrams, (1, 2) means unigrams and bigrams, and (2, 2) means only bigrams. Only applies if `analyzer` is not callable.\n",
    "\n",
    "> - I strongly recommend going through all the hyper-parameters of `CountVectorizer()` at the following link:\n",
    "\n",
    "[https://scikit-learn.org/stable/modules/generated/sklearn.feature\\_extraction.text.CountVectorizer.html](https://www.google.com/url?sa=E&source=gmail&q=https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Bag of N-Grams Encoding\n",
    "\n",
    "### a. Conceptual Understanding\n",
    "- In the simple BoW model, the vocabulary consists of single unique words of the corpus, and its limitation is that the ordering of words is not captured.\n",
    "- A `Bag of n-grams model` is quite similar to the BoW model, and it represents a text document as an unordered collection of its n-grams (a contiguous sequence of **n items** from a given sample of text or speech).\n",
    "- An n-gram of size 1 is referred to as a \"unigram\"; size 2 is a \"bigram\"; size 3 is a \"trigram,\" and so on.\n",
    "- The formula to calculate the count of n-grams in a document is:\n",
    "  $\n",
    "  \\text{X} - (\\text{N} - 1)\n",
    "  $\n",
    "  where **X** is the number of words in a given document and **N** is the number of words in n-grams.\n",
    "\n",
    "### Example:\n",
    "- Using BoW representation, the two sentences:\n",
    "  - *Ali YouTube lectures are good*\n",
    "  - *Ali YouTube lectures are not good*\n",
    "  will be very similar.\n",
    "- Using bigram, the two bigrams **are good** in the first sentence and **not good** in the second sentence will make the difference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Consider the following corpus that consists of three documents consisting of five, ten, and five words respectively:\n",
    "\n",
    "    ```boldtext\n",
    "    doc1 = [\"Ali YouTube lectures are amazing\"]\n",
    "    doc2 = [\"I like YouTube lectures and Khurram also like YouTube lectures\"]\n",
    "    doc3 = [\"Ali YouTube lectures are great\"]\n",
    "    ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The vocabulary of bi-grams in this case consists of ten words, which can be represented as a dictionary as shown below:\n",
    "    ```boldtext\n",
    "    vocab = {\n",
    "        'also like': 0,\n",
    "        'and khurram': 1,\n",
    "        'are amazing': 2,\n",
    "        'are great': 3,\n",
    "        'ali youtube': 4,\n",
    "        'khurram also': 5,\n",
    "        'lectures and': 6,\n",
    "        'lectures are': 7,\n",
    "        'like youtube': 8,\n",
    "        'youtube lectures': 9\n",
    "    }\n",
    "    ```\n",
    "\n",
    "**Bag of Bi-grams**\n",
    "| Document | also like | and khurram | are amazing | are great | ali youtube | khurram also | lectures and | lectures are | like youtube | youtube lectures |\n",
    "|----------|-----------|-------------|-------------|-----------|-------------|--------------|--------------|--------------|--------------|------------------|\n",
    "| **doc1** | 0         | 0           | 1           | 0         | 1           | 0            | 0            | 1            | 0            | 1                |\n",
    "| **doc2** | 1         | 1           | 0           | 0         | 0           | 1            | 1            | 0            | 2            | 2                |\n",
    "| **doc3** | 0         | 0           | 0           | 1         | 1           | 0            | 0            | 1            | 0            | 1                |\n",
    "\n",
    "\n",
    "- The vocabulary of tri-grams in this case consists of ten words, which can be represented as a dictionary as shown below:\n",
    "\n",
    "    ```boldtext\n",
    "    vocab = {\n",
    "        'also like youtube': 0,\n",
    "        'and khurram also': 1,\n",
    "        'ali youtube lectures': 2,\n",
    "        'khurram also like': 3,\n",
    "        'lectures and khurram': 4,\n",
    "        'lectures are amazing': 5,\n",
    "        'lectures are great': 6,\n",
    "        'like youtube lectures': 7,\n",
    "        'youtube lectures and': 8,\n",
    "        'youtube lectures are': 9\n",
    "    }\n",
    "    ```\n",
    "\n",
    "**Bag of Tri-grams**\n",
    "\n",
    "| Document | also like youtube | and khurram also | ali youtube lectures | khurram also like | lectures and khurram | lectures are amazing | lectures are great | like youtube lectures | youtube lectures and | youtube lectures are |\n",
    "|----------|-------------------|------------------|-----------------------|-------------------|----------------------|----------------------|--------------------|-----------------------|-----------------------|-----------------------|\n",
    "| **doc1** | 0                 | 0                | 1                     | 0                 | 0                    | 1                    | 0                  | 0                     | 0                     | 1                     |\n",
    "| **doc2** | 1                 | 1                | 0                     | 1                 | 1                    | 0                    | 0                  | 1                     | 0                     | 0                     |\n",
    "| **doc3** | 0                 | 0                | 1                     | 0                 | 0                    | 0                    | 1                  | 0                     | 0                     | 1                     |\n",
    "\n",
    "- **Advantages of N-Grams**\n",
    "\n",
    "    - `Captures Semantic Meaning:` As we use bi-grams or tri-grams, it takes a sequence of sentences which makes it easy for finding word relationships.\n",
    "    - `Intuitive and Easy to Implement:` Implementation of N-grams is straightforward with a little bit of modification in the Bag of Words approach.\n",
    "\n",
    "- **Disadvantages of Bag of N-Grams Encoding**\n",
    "    - As we move from unigram to N-gram, the dimension of vector formation or vocabulary increases, due to which it takes more time in computation and prediction.\n",
    "    - Still no solution for out-of-vocabulary terms – we do not have a way other than ignoring the new words in a new sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b. Creating Bag of N-Grams using `CountVectorizer`\n",
    "\n",
    "- To create a Bag of N-Grams, we can use the `ngram_range` argument of the `CountVectorizer` method:\n",
    "\n",
    "    ```boldtext\n",
    "    cv = sklearn.feature_extraction.text.CountVectorizer(ngram_range=(1,1))\n",
    "    ```\n",
    "#### Explanation:\n",
    "- **Uni-grams**: Single words from the text are treated as tokens.\n",
    "- **Bi-grams**: Consecutive pairs of words are treated as tokens.\n",
    "- **Tri-grams**: Consecutive triplets of words are treated as tokens.\n",
    "- **Combined N-Grams**: Tokens are generated by combining multiple ranges of N-Grams (e.g., uni-grams, bi-grams, and tri-grams together).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example of Bi-Grams Using CountVectorizer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [\n",
    "    \"Ali YouTube lectures are amazing\",\n",
    "    \"I like YouTube lectures and Khurram also like YouTube lectures\",\n",
    "    \"Ali YouTube lectures are great\"\n",
    "]\n",
    "\n",
    "# Import CountVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Create an object of CountVectorizer with ngram_range for bi-grams\n",
    "cv = CountVectorizer(ngram_range=(2, 2))\n",
    "\n",
    "# Fit and transform the corpus to create a bag-of-words representation\n",
    "bow = cv.fit_transform(corpus)\n",
    "\n",
    "# Print the vocabulary\n",
    "print(cv.vocabulary_)\n",
    "\n",
    "# Print the bag-of-words matrix as an array\n",
    "print(\"\\n\\n\")\n",
    "print(bow.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Convert the matrix to a DataFrame\n",
    "dtm1 = pd.DataFrame(data=bow.toarray(), columns=cv.get_feature_names_out())\n",
    "\n",
    "# Display the DataFrame\n",
    "dtm1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example of Tri-Grams Using CountVectorizer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [\n",
    "    \"Ali YouTube lectures are amazing\",\n",
    "    \"I like YouTube lectures and Khurram also like YouTube lectures\",\n",
    "    \"Ali YouTube lectures are great\"\n",
    "]\n",
    "\n",
    "# Import CountVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Create an object of CountVectorizer with ngram_range for bi-grams\n",
    "cv = CountVectorizer(ngram_range=(3, 3))\n",
    "\n",
    "# Fit and transform the corpus to create a bag-of-words representation\n",
    "bow = cv.fit_transform(corpus)\n",
    "\n",
    "# Print the vocabulary\n",
    "print(cv.vocabulary_)\n",
    "\n",
    "# Print the bag-of-words matrix as an array\n",
    "print(\"\\n\\n\")\n",
    "print(bow.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Convert the matrix to a DataFrame\n",
    "dtm1 = pd.DataFrame(data=bow.toarray(), columns=cv.get_feature_names_out())\n",
    "\n",
    "# Display the DataFrame\n",
    "dtm1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Term Frequency -inverse Document Frequency (TF-IDF)\n",
    "\n",
    "\n",
    "## a. Conceptual Understanding\n",
    "\n",
    "- In the `Bag of Words` representation of a document, the values of the vector are the number of times a particular word appears in a document, but it does not capture the importance of a word in a document.\n",
    "- In simple terms, the Bag of Words approach treats every word equally, irrespective of its actual importance.\n",
    "- So, BoW gives more importance to some unimportant words that appear more frequently in a document. For example, words like `since`, `as`, `can`, `any`, `and`, `the`, `of`, `it`, `they` can have high frequency but are not important.\n",
    "- This will take the attention of the machine learning model away from less frequent but more important words.\n",
    "- **TF-IDF** stands for **Term Frequency (TF)** times **Inverse Document Frequency (IDF)**, and it is used to address this issue:\n",
    "  - **Term Frequency (TF)**: Tells us how important a term is in a particular document by assigning more weight to a term that is appearing more frequently in a dataset.\n",
    "  - **Document Frequency (IDF)**: Tells us how important a term is in the entire corpus of documents. The intuition behind taking its inverse is that the more common a word is across all documents, the lesser its importance for the current document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (i) Term Frequency (TF)\n",
    "\n",
    "- **Term Frequency (TF)** tells us the count of a term in a specific document and thus tells us how important a term is in a particular document.\n",
    "- In literature, there are two different formulae for computing **TF**, as shown below:\n",
    "\n",
    "$$\n",
    "TF(t, d) = \\frac{\\text{Number of times term (t) occurs in document (d)}}{\\text{Total number of terms in document (d)}}\n",
    "$$\n",
    "\n",
    "$$\n",
    "TF(t, d) = \\frac{\\text{Number of times term (t) occurs in document (d)}}{\\text{Frequency of most common term in document (d)}}\n",
    "$$\n",
    "- Consider a corpus of three documents as shown below:\n",
    "\n",
    "    ```boldtext\n",
    "    doc1 = [\"Ali YouTube lectures are amazing\"]\n",
    "    doc2 = [\"I like YouTube lectures and Khurram also like YouTube lectures\"]\n",
    "    doc3 = [\"Ali YouTube lectures are great\"]\n",
    "    ```\n",
    "\n",
    "> The `TfidfVectorizer()` method of sklearn uses the first formula.\n",
    "\n",
    "**Term Frequencies of Each Term in Each Document**\n",
    "\n",
    "| Term       | also | amazing | and | are | ali | great | khurram | lectures | like | youtube |\n",
    "|------------|------|---------|-----|-----|------|-------|---------|----------|------|---------|\n",
    "| **doc1**   | 0    | 1       | 0   | 1   | 1    | 0     | 0       | 1        | 0    | 1       |\n",
    "| **doc2**   | 1    | 0       | 1   | 0   | 0    | 0     | 1       | 2        | 2    | 2       |\n",
    "| **doc3**   | 0    | 0       | 0   | 1   | 1    | 1     | 0       | 1        | 0    | 1       |\n",
    "\n",
    "\n",
    "**Document Analysis**\n",
    "\n",
    "- **Doc1**:\n",
    "  - Contains a total of 5 words, each appearing once.\n",
    "  - Represented as a vector of size 10 (count of vocabulary), having 5 non-zero values.\n",
    "\n",
    "- **Doc2**:\n",
    "  - Contains a total of 10 words, with 6 unique words (excluding the single character \"I\").\n",
    "  - The words *like*, *YouTube*, and *lectures* appear twice, while all other words appear once.\n",
    "  - Represented as a vector of size 10 (count of vocabulary), having 6 non-zero values.\n",
    "\n",
    "- **Doc3**:\n",
    "  - Contains a total of 5 words, each appearing once.\n",
    "  - Represented as a vector of size 10 (count of vocabulary), having 5 non-zero values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (ii) Inverse Document Frequency (IDF)\n",
    "\n",
    "- **Document Frequency** tells us how important a term is in the entire corpus of documents.\n",
    "- To penalize frequently occurring words across all the documents, we take the **Inverse of Document Frequency**.\n",
    "- This way, the IDF of rare words in the corpus will be large, while the IDF of very common words in the corpus will be close to zero.\n",
    "- In literature, the most common formula for computing IDF is:\n",
    "\n",
    "\n",
    "$$\\text{IDF}(t, D) = \\log_e \\frac{n}{df(d, t)}$$\n",
    "\n",
    "\n",
    "Where:\n",
    "- \\( n \\) = Total number of documents in the corpus.\n",
    "- \\( df(d, t) \\) = Number of documents in which term \\( t \\) appears.\n",
    "\n",
    "- If a word is appearing in only one document (large value of \\( n \\)), then the IDF value of that term will be very large. Therefore, we take the natural logarithm to dampen its effect.\n",
    "\n",
    "- **Sklearn's `TfidfVectorizer()`** uses the following formula when the argument `smooth_idf=True`:\n",
    "\n",
    "$$\\text{IDF}(t, D) = 1 + \\log_e \\frac{1 + n}{1 + df(d, t)}$$\n",
    "\n",
    "#### Why `smooth_idf=True`?\n",
    "\n",
    "- The constant \"1\" is added to the numerator and denominator to prevent zero divisions.\n",
    "- Adding \"1\" ensures terms with zero IDF (i.e., terms that occur in all documents) will not be entirely ignored.\n",
    "\n",
    "\n",
    "**Examples:**\n",
    "\n",
    "$$\\text{IDF}(\\text{Ali}, D) = 1 + \\log_e \\frac{1 + 3}{1 + 2} = 1 + \\log_e(1.333333) = 1 + 0.28768 = 1.28768$$\n",
    "\n",
    "\n",
    "$$\\text{IDF}(\\text{YouTube}, D) = 1 + \\log_e \\frac{1 + 3}{1 + 3} = 1 + \\log_e(1) = 1$$\n",
    "\n",
    "\n",
    "\n",
    "**Inverse Document Frequencies (common for all the documents)**\n",
    "\n",
    "\n",
    "| Term       | also       | amaizing   | and        | are        | ali        | great      | khurram    | lectures   | like       | youtube    |\n",
    "|------------|------------|------------|------------|------------|------------|------------|------------|------------|------------|------------|\n",
    "| Corpus     | 1.69314718 | 1.69314718 | 1.69314718 | 1.28768207 | 1.28768207 | 1.69314718 | 1.69314718 | 1          | 1.69314718 | 1          |\n",
    "\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "\n",
    "> - The **Term Frequency (TF)** varies for each term in each document, but **Inverse Document Frequency (IDF)** values remain the same for all terms across all the documents in the corpus.\n",
    "> - A term that appears in all the documents (e.g., \"youtube\") is not given a zero IDF because of the \"+1\" added in the formula. This ensures that the term is not entirely ignored in the overall calculation of TF-IDF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (iii) Term Frequency-Inverse Document Frequency (TFIDF)\n",
    "- The **TFIDF score** can be calculated using the following formula:\n",
    "\n",
    "$$TFIDF = TF \\times IDF$$\n",
    "\n",
    "**TFIDF Table**\n",
    "\n",
    "| Term   | also       | amaizing   | and        | are        | ali        | great      | khurram    | lectures   | like       | youtube    |\n",
    "|--------|------------|------------|------------|------------|------------|------------|------------|------------|------------|------------|\n",
    "| doc1   | 0          | 1.69314718 | 0          | 1.28768207 | 1.28768207 | 0          | 0          | 1          | 0          | 1          |\n",
    "| doc2   | 1.69314718 | 0          | 1.69314718 | 0          | 0          | 0          | 1.69314718 | 2          | 3.38629436 | 2          |\n",
    "| doc3   | 0          | 0          | 0          | 1.28768207 | 1.28768207 | 1.69314718 | 0          | 1          | 0          | 1          |\n",
    "\n",
    "\n",
    "### (iv) Normalizing TFIDF Values\n",
    "- To avoid large documents in the corpus dominating smaller ones, normalization is applied to each row in the sparse matrix. This is achieved by calculating the Euclidean norm for each document.\n",
    "- The **Euclidean Norm** for the three documents is:\n",
    "  - **Doc1**: 2.86059\n",
    "  - **Doc2**: 5.29785\n",
    "  - **Doc3**: 2.86059\n",
    "\n",
    "\n",
    "$$Normalized \\, TFIDF = \\frac{TFIDF}{\\sqrt{\\sum_{i=1}^{n}(TFIDF^2)}}$$\n",
    "\n",
    "**Normalized TFIDF Table**\n",
    "\n",
    "| Term   | also       | amaizing   | and        | are        | ali        | great      | khurram    | lectures   | like       | youtube    |\n",
    "|--------|------------|------------|------------|------------|------------|------------|------------|------------|------------|------------|\n",
    "| doc1   | 0          | 0.59188659 | 0          | 0.45014501 | 0.45014501 | 0          | 0          | 0.34957775 | 0          | 0.34957775 |\n",
    "| doc2   | 0.31959128 | 0          | 0.31959128 | 0          | 0          | 0          | 0.31959128 | 0.37751152 | 0.63918256 | 0.37751152 |\n",
    "| doc3   | 0          | 0          | 0          | 0.45014501 | 0.45014501 | 0.59188659 | 0          | 0.34957775 | 0          | 0.34957775 |\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "- **The higher the TFIDF score, the more relevant the term is in that document.**\n",
    "  - In **doc1**, the term **\"amaizing\"** is the most relevant term as it has the highest TFIDF value (**0.59188659**).\n",
    "  - In **doc2**, the term **\"like\"** is the most relevant term as it has the highest TFIDF value (**0.63918256**).\n",
    "  - In **doc3**, the term **\"great\"** is the most relevant term as it has the highest TFIDF value (**0.59188659**)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## b. Creating TFIDF using `TfidfVectorizer`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [\n",
    "    \"Ali YouTube lectures are amazing\",\n",
    "    \"I like YouTube lectures and Khurram also like YouTube lectures\",\n",
    "    \"Ali YouTube lectures are great\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sklearn.feature_extraction.text.TfidfVectorizer"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create an instance of TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf_vec = TfidfVectorizer()\n",
    "type(tfidf_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = tfidf_vec.fit_transform(corpus) # generates vocabulary dictionary and returns a DTM having TF-IDF values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ali': 0, 'youtube': 9, 'lectures': 7, 'are': 4, 'amazing': 2, 'like': 8, 'and': 3, 'khurram': 6, 'also': 1, 'great': 5}\n"
     ]
    }
   ],
   "source": [
    "# to print the vocabulary\n",
    "print(tfidf_vec.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ali' 'also' 'amazing' 'and' 'are' 'great' 'khurram' 'lectures' 'like'\n",
      " 'youtube']\n"
     ]
    }
   ],
   "source": [
    "print(tfidf_vec.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Note that single characters are not included in the vocabulary.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 10)"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<3x10 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 16 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.45014501, 0.        , 0.59188659, 0.        , 0.45014501,\n",
       "        0.        , 0.        , 0.34957775, 0.        , 0.34957775],\n",
       "       [0.        , 0.31959128, 0.        , 0.31959128, 0.        ,\n",
       "        0.        , 0.31959128, 0.37751152, 0.63918256, 0.37751152],\n",
       "       [0.45014501, 0.        , 0.        , 0.        , 0.45014501,\n",
       "        0.59188659, 0.        , 0.34957775, 0.        , 0.34957775]])"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# since the tfidf is sparse matrix, we need to convert it to a dense matrix by using numpy toarray() or todense() method\n",
    "tfidf.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Let us save the corpus as a Document Term Matrix of TF-IDF values**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ali</th>\n",
       "      <th>also</th>\n",
       "      <th>amazing</th>\n",
       "      <th>and</th>\n",
       "      <th>are</th>\n",
       "      <th>great</th>\n",
       "      <th>khurram</th>\n",
       "      <th>lectures</th>\n",
       "      <th>like</th>\n",
       "      <th>youtube</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.450145</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.591887</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.450145</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.349578</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.349578</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.319591</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.319591</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.319591</td>\n",
       "      <td>0.377512</td>\n",
       "      <td>0.639183</td>\n",
       "      <td>0.377512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.450145</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.450145</td>\n",
       "      <td>0.591887</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.349578</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.349578</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        ali      also   amazing       and       are     great   khurram  \\\n",
       "0  0.450145  0.000000  0.591887  0.000000  0.450145  0.000000  0.000000   \n",
       "1  0.000000  0.319591  0.000000  0.319591  0.000000  0.000000  0.319591   \n",
       "2  0.450145  0.000000  0.000000  0.000000  0.450145  0.591887  0.000000   \n",
       "\n",
       "   lectures      like   youtube  \n",
       "0  0.349578  0.000000  0.349578  \n",
       "1  0.377512  0.639183  0.377512  \n",
       "2  0.349578  0.000000  0.349578  "
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dtm2 = pd.DataFrame(data=tfidf.toarray(), columns=tfidf_vec.get_feature_names_out())\n",
    "dtm2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Advantage:**\n",
    "- The TFIDF technique of vectorization is mainly used in **Information Retrieval**, such as in Google Search Engines.\n",
    "\n",
    "**Limitations: *(Almost same as BoW)***\n",
    "- **Size of the Vector**: Depends on the overall size of the vocabulary (quite large), thus increasing the number of dimensions.\n",
    "- **Sparsity Exists**: Many entries in the vector remain zero.\n",
    "- **Out of Vocabulary Problem**: If a new word appears, it cannot be vectorized.\n",
    "- **Increased Dimensions**: The number of dimensions increases with a larger vocabulary.\n",
    "- **Semantic Meanings Are Not Completely Captured**:\n",
    "  - **Ordering of Words Is Ignored**: The sequence of words is not considered.\n",
    "  - **Two Very Similar Vectors Can Convey Completely Different Meanings**: This can lead to misrepresentation of the context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Word Embeddings\n",
    "\n",
    "### a. Conceptual Understanding\n",
    "- In Bag of Words and TF-IDF encodings, every word is treated as an individual entity, and semantics are completely ignored.\n",
    "- These vectorization techniques work fine for NLP tasks like **Text Generation** and **Classification**.\n",
    "- However, Bag-of-Words and TFIDF encodings won’t be as effective for other NLP tasks like **Sentiment Analysis**, **Machine Translation**, and **Question Answering**, where a deeper understanding of the context is required to achieve great results.\n",
    "- For this, we turn to **Word Embeddings**, a featurized word-level representation capable of capturing the semantic meanings of words.\n",
    "- **Word Embeddings** are techniques that map a single **word** as well as an entire **document** to a dense vector of fixed size (50 to 300 dimensions) that captures the semantic meanings of words.\n",
    "\n",
    "#### Word Embedding Techniques:\n",
    "1. **Prediction-Based (Don’t Count, Predict)**:\n",
    "   - **Word2Vec** by Google (2013): [Paper](https://arxiv.org/pdf/1301.3781.pdf)\n",
    "   - **FastText** by Facebook (2015): [Paper](https://arxiv.org/pdf/1607.01759.pdf)\n",
    "2. **Frequency/Count-Based**:\n",
    "   - **Global Vectors (GloVe)** by Stanford (2014): [Paper](https://nlp.stanford.edu/pubs/glove.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b. Word2Vec\n",
    "- **Word2Vec** technique was released in 2013 by Google researchers that uses the power of a simple Neural Network to generate word embeddings.\n",
    "- It is a contextually aware word embedding technique that uses a simple Neural Network to generate word embeddings.\n",
    "- It converts a word into a vector of real numbers (300 or maybe 400 dimensions).\n",
    "- **Two Approaches to Train a Word2Vec Model**:\n",
    "  - **CBOW**: Use the context words to predict the target word.\n",
    "  - **Skip-Gram**: Use a word to predict target context words.\n",
    "  \n",
    "<img align=\"right\" width=\"800\"  src=\"images/word2vec.jpeg\" >\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example:**\n",
    "\n",
    "- Bag of Words (BOW) and TFIDF treat `king`, `queen`, `woman`, and `princess` as completely different words.\n",
    "- **Word Embedding techniques** will capture the semantic meanings of these words and represent them as vectors that are quite close or similar to each other.\n",
    "\n",
    "<img align=\"right\" width=\"800\"  src=\"images/word2vectors.png\" >"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c. `word2vec` using spaCy `en_core_web_lg` Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_lg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "342918"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(nlp.vocab.vectors) # number of words in the vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(342918, 300)"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.vocab.vectors.shape # dimension of the word vectors, it means that each word has 300 dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Vector Representation of Words**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1.7310e-01,  2.0663e-01,  1.6543e-02, -3.1026e-01,  1.9719e-02,\n",
       "        2.7791e-01,  1.2283e-01, -2.6328e-01,  1.2522e-01,  3.1894e+00,\n",
       "       -1.6291e-01, -8.8759e-02,  3.3067e-03, -2.9483e-03, -3.4398e-01,\n",
       "        1.2779e-01, -9.4536e-02,  4.3467e-01,  4.9742e-01,  2.5068e-01,\n",
       "       -2.0901e-01, -5.8931e-01,  6.1615e-02,  1.0434e-01,  2.4424e-01,\n",
       "       -2.9120e-01,  3.0746e-01,  3.6276e-01,  7.1151e-01, -8.0523e-02,\n",
       "       -5.9524e-01,  3.4834e-01, -3.3048e-01,  7.0316e-02,  5.3329e-01,\n",
       "       -2.9081e-01,  1.3459e-01, -3.9856e-01, -3.2435e-01,  1.1867e-01,\n",
       "       -1.4938e-01, -3.8256e-01,  3.3116e-01, -3.1488e-01, -9.4491e-02,\n",
       "       -6.1319e-02,  1.5518e-01, -2.5523e-01, -1.1813e-01,  2.5296e-01,\n",
       "       -9.5174e-02, -1.6596e-01, -1.0840e-01,  8.8803e-02,  2.0890e-01,\n",
       "        4.3981e-01,  1.0476e-03, -4.0666e-02,  2.6487e-01, -6.1009e-01,\n",
       "       -1.4405e-01, -8.1185e-02,  7.5475e-03,  2.3373e-01, -2.7772e-02,\n",
       "       -2.9315e-01, -1.1744e-01, -8.3193e-02, -2.3768e-01,  1.5735e-01,\n",
       "       -5.6408e-01, -2.9323e-01,  1.6387e-02, -7.8160e-02, -2.1301e-01,\n",
       "       -1.7845e-01, -3.0769e-01, -4.6203e-01, -2.5220e-03,  4.2973e-01,\n",
       "       -4.5647e-02,  4.3434e-01,  2.9144e-01, -2.0231e-01,  6.5217e-02,\n",
       "       -4.2321e-01,  7.1227e-01, -5.5498e-01, -3.6118e-01, -1.4849e-01,\n",
       "       -1.0082e-01, -1.0045e-01, -1.7666e-01,  2.7141e-02, -1.4148e-01,\n",
       "       -5.2112e-01, -4.8785e-02, -2.9672e-01, -1.4418e-01, -2.8042e-01,\n",
       "       -6.8216e-02,  6.4455e-01,  9.4982e-02,  8.4994e-02,  8.2954e-02,\n",
       "       -2.5255e-01, -4.6606e-01,  2.7432e-01,  1.4998e-01,  5.6889e-02,\n",
       "        3.4382e-01, -3.5301e-01, -4.0666e-01,  1.3636e-02,  6.3070e-02,\n",
       "        3.4924e-01,  1.7517e-01, -1.7623e-01, -8.0157e-02,  1.2179e-01,\n",
       "       -1.4025e-01, -2.6541e-01,  1.7016e-01,  1.2596e-01, -2.1399e-01,\n",
       "       -2.1077e-01, -4.6861e-01, -2.2138e-01,  1.6753e-03, -1.4527e-01,\n",
       "        1.1256e-01, -2.4644e-02, -2.5005e-01, -4.3965e-01,  4.6923e-01,\n",
       "        2.0156e-01,  3.2739e-01,  6.8498e-02, -2.0121e-01, -7.8691e-02,\n",
       "       -2.6755e+00,  1.4326e-01,  6.9114e-02,  3.6917e-01, -1.1858e-01,\n",
       "       -2.1332e-01,  2.4918e-01, -1.2622e-01,  5.5950e-02, -2.8807e-01,\n",
       "       -5.9173e-01, -9.0193e-02, -1.8155e-01, -3.3470e-01, -7.4857e-02,\n",
       "        1.1898e-01, -4.1698e-01, -1.9773e-01,  1.9724e-01,  1.1124e-01,\n",
       "        2.6915e-01, -4.3901e-01, -5.8556e-02,  4.2020e-01,  2.7380e-01,\n",
       "       -9.7767e-02,  7.5527e-02, -2.1164e-01, -1.9093e-01,  3.5376e-01,\n",
       "       -6.9182e-01, -2.5014e-02,  5.8887e-02, -1.7279e-01, -4.4822e-01,\n",
       "       -1.5286e-01, -2.1873e-01,  2.2586e-01, -1.3494e-01,  7.7586e-03,\n",
       "       -2.8582e-01,  6.0300e-02, -6.1573e-03, -3.0737e-01, -8.2510e-01,\n",
       "        5.8975e-01, -4.1435e-01,  9.8575e-02, -8.0579e-02, -6.0510e-01,\n",
       "       -3.1138e-01,  2.3793e-01, -3.3706e-01,  9.7126e-02,  4.1277e-01,\n",
       "        2.7025e-01, -7.1067e-02, -5.1049e-01, -8.0376e-02, -1.6188e-01,\n",
       "        1.3121e-02,  2.1678e-01, -2.7958e-01, -1.9755e-01, -3.4668e-01,\n",
       "        1.0901e-01, -1.5878e-01,  1.1535e-02, -2.5060e-02, -2.4669e-01,\n",
       "        2.7795e-02, -4.6083e-01, -1.6082e-01, -1.6443e-01,  3.4003e-01,\n",
       "       -6.1288e-01,  1.3028e-03, -1.2118e-01,  3.8965e-01, -4.3394e-01,\n",
       "        6.6747e-02,  2.4109e-01, -3.1253e-01,  2.7997e-01,  1.4047e-01,\n",
       "       -1.6265e-02, -6.0792e-02,  6.5612e-01, -7.1277e-02,  8.2271e-02,\n",
       "        8.5240e-02,  1.5606e-01, -2.1927e-01, -1.2083e-01, -2.0386e-01,\n",
       "       -3.9694e-03, -4.3643e-02,  4.2532e-01, -3.3641e-01,  3.0292e-01,\n",
       "        2.9297e-01,  1.2963e-01,  1.5872e-01, -3.1301e-01,  2.6109e-01,\n",
       "        2.4433e-01, -1.9605e-02, -4.4197e-01,  4.5351e-01,  2.2184e-01,\n",
       "        3.6002e-02,  3.0979e-01,  2.8024e-02,  2.9232e-01,  8.8706e-02,\n",
       "        1.3376e-01,  5.6731e-01, -1.7374e-01,  3.7015e-01, -2.9521e-01,\n",
       "        3.4110e-02,  4.1392e-01,  1.5641e-02, -3.1128e-01, -3.4823e-01,\n",
       "        3.3560e-01,  1.7200e-01,  4.1568e-01, -2.6028e-01,  3.3601e-01,\n",
       "       -4.1509e-03, -1.6774e-02, -2.6867e-01,  5.0892e-02, -1.2670e-01,\n",
       "        4.6916e-01, -1.1842e-01,  2.9402e-01,  3.5544e-01,  7.3157e-02,\n",
       "       -1.9305e-01,  2.3060e-01, -2.6933e-01,  6.2013e-02, -5.4700e-02,\n",
       "       -3.8535e-01, -1.3039e-01, -8.4836e-02,  9.8583e-02, -1.5403e-01,\n",
       "       -3.6347e-01, -1.9985e-01, -2.2663e-01, -5.3781e-01, -2.4666e-01,\n",
       "        1.9266e-02,  2.1319e-01,  1.6665e-01, -3.8341e-01, -7.3803e-02],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp(u\"man\").vector # here u is a Unicode character"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp(u\"ehtisham\").vector # here u is a Unicode character"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Out of Vocabulary Problem and L2 Norms**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['_',\n",
       " '__bytes__',\n",
       " '__class__',\n",
       " '__delattr__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__getstate__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__len__',\n",
       " '__lt__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__pyx_vtable__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__unicode__',\n",
       " 'ancestors',\n",
       " 'check_flag',\n",
       " 'children',\n",
       " 'cluster',\n",
       " 'conjuncts',\n",
       " 'dep',\n",
       " 'dep_',\n",
       " 'doc',\n",
       " 'ent_id',\n",
       " 'ent_id_',\n",
       " 'ent_iob',\n",
       " 'ent_iob_',\n",
       " 'ent_kb_id',\n",
       " 'ent_kb_id_',\n",
       " 'ent_type',\n",
       " 'ent_type_',\n",
       " 'get_extension',\n",
       " 'has_dep',\n",
       " 'has_extension',\n",
       " 'has_head',\n",
       " 'has_morph',\n",
       " 'has_vector',\n",
       " 'head',\n",
       " 'i',\n",
       " 'idx',\n",
       " 'iob_strings',\n",
       " 'is_alpha',\n",
       " 'is_ancestor',\n",
       " 'is_ascii',\n",
       " 'is_bracket',\n",
       " 'is_currency',\n",
       " 'is_digit',\n",
       " 'is_left_punct',\n",
       " 'is_lower',\n",
       " 'is_oov',\n",
       " 'is_punct',\n",
       " 'is_quote',\n",
       " 'is_right_punct',\n",
       " 'is_sent_end',\n",
       " 'is_sent_start',\n",
       " 'is_space',\n",
       " 'is_stop',\n",
       " 'is_title',\n",
       " 'is_upper',\n",
       " 'lang',\n",
       " 'lang_',\n",
       " 'left_edge',\n",
       " 'lefts',\n",
       " 'lemma',\n",
       " 'lemma_',\n",
       " 'lex',\n",
       " 'lex_id',\n",
       " 'like_email',\n",
       " 'like_num',\n",
       " 'like_url',\n",
       " 'lower',\n",
       " 'lower_',\n",
       " 'morph',\n",
       " 'n_lefts',\n",
       " 'n_rights',\n",
       " 'nbor',\n",
       " 'norm',\n",
       " 'norm_',\n",
       " 'orth',\n",
       " 'orth_',\n",
       " 'pos',\n",
       " 'pos_',\n",
       " 'prefix',\n",
       " 'prefix_',\n",
       " 'prob',\n",
       " 'rank',\n",
       " 'remove_extension',\n",
       " 'right_edge',\n",
       " 'rights',\n",
       " 'sent',\n",
       " 'sent_start',\n",
       " 'sentiment',\n",
       " 'set_extension',\n",
       " 'set_morph',\n",
       " 'shape',\n",
       " 'shape_',\n",
       " 'similarity',\n",
       " 'subtree',\n",
       " 'suffix',\n",
       " 'suffix_',\n",
       " 'tag',\n",
       " 'tag_',\n",
       " 'tensor',\n",
       " 'text',\n",
       " 'text_with_ws',\n",
       " 'vector',\n",
       " 'vector_norm',\n",
       " 'vocab',\n",
       " 'whitespace_']"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = nlp(u\"lion cat ehtisham\")\n",
    "dir(words[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lion ---> True ---> 6.5120897 ---> False\n",
      "cat ---> True ---> 6.6808186 ---> False\n",
      "ehtisham ---> False ---> 0.0 ---> True\n"
     ]
    }
   ],
   "source": [
    "for word in words:\n",
    "    print(word.text, '--->', word.has_vector, '--->', word.vector_norm, '--->', word.is_oov)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cosine Similarity/Cosine Distance among Vectors**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python ---> python ---> 1.0\n",
      "python ---> cat ---> 0.3270457983016968\n",
      "python ---> pet ---> 0.21873925626277924\n",
      "cat ---> python ---> 0.3270457983016968\n",
      "cat ---> cat ---> 1.0\n",
      "cat ---> pet ---> 0.7505456209182739\n",
      "pet ---> python ---> 0.21873925626277924\n",
      "pet ---> cat ---> 0.7505456209182739\n",
      "pet ---> pet ---> 1.0\n"
     ]
    }
   ],
   "source": [
    "words = nlp(u\"python cat pet\")\n",
    "for word1 in words:\n",
    "    for word2 in words:\n",
    "        print(word1.text, '--->', word2.text, '--->', word1.similarity(word2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "love ---> love ---> 1.0\n",
      "love ---> hate ---> 0.6393099427223206\n",
      "hate ---> love ---> 0.6393099427223206\n",
      "hate ---> hate ---> 1.0\n"
     ]
    }
   ],
   "source": [
    "words = nlp(u\"love hate\")\n",
    "for word1 in words:\n",
    "    for word2 in words:\n",
    "        print(word1.text, '--->', word2.text, '--->', word1.similarity(word2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Compute Cosine Similarity and Cosine distance using Sklearn**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.63931]]\n",
      "[[0.36069]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn. metrics.pairwise import cosine_similarity, cosine_distances\n",
    "\n",
    "words = nlp(u\"love hate\")\n",
    "print(cosine_similarity(words[0].vector.reshape(1, -1), words[1].vector.reshape(1, -1)))\n",
    "print(cosine_distances(words[0].vector.reshape(1, -1), words[1].vector.reshape(1, -1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.63931]]\n",
      "[[0.36069]]\n"
     ]
    }
   ],
   "source": [
    "print(cosine_similarity([words[0].vector], [words[1].vector]))\n",
    "print(cosine_distances([words[0].vector], [words[1].vector]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.display import HTML\n",
    "\n",
    "style = \"\"\"\n",
    "    <style>\n",
    "        body {\n",
    "            background-color: #f2fff2;\n",
    "        }\n",
    "        h1 {\n",
    "            text-align: center;\n",
    "            font-weight: bold;\n",
    "            font-size: 36px;\n",
    "            color: #4295F4;\n",
    "            text-decoration: underline;\n",
    "            padding-top: 15px;\n",
    "        }\n",
    "        \n",
    "        h2 {\n",
    "            text-align: left;\n",
    "            font-weight: bold;\n",
    "            font-size: 30px;\n",
    "            color: #4A000A;\n",
    "            text-decoration: underline;\n",
    "            padding-top: 10px;\n",
    "        }\n",
    "        \n",
    "        h3 {\n",
    "            text-align: left;\n",
    "            font-weight: bold;\n",
    "            font-size: 30px;\n",
    "            color: #f0081e;\n",
    "            text-decoration: underline;\n",
    "            padding-top: 5px;\n",
    "        }\n",
    "\n",
    "        \n",
    "        p {\n",
    "            text-align: center;\n",
    "            font-size: 12 px;\n",
    "            color: #0B9923;\n",
    "        }\n",
    "    </style>\n",
    "\"\"\"\n",
    "\n",
    "html_content = \"\"\"\n",
    "<h1>Hello</h1>\n",
    "<p>Hello World</p>\n",
    "<h2> Hello</h2>\n",
    "<h3> World </h3>\n",
    "\"\"\"\n",
    "\n",
    "HTML(style + html_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
