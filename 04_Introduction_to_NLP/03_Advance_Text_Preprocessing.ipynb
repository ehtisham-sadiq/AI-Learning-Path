{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d162b058",
   "metadata": {},
   "source": [
    "<h1 align=\"center\">Advance Text Pre-processing</h1>\n",
    "<h3><div align=\"right\">Ehtisham Sadiq</div></h3>    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19b0c47d",
   "metadata": {},
   "source": [
    "<img align=\"center\" width=\"900\"  src=\"images/phase2.png\"  > "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e23a6901",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2c17026",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9efdaacd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1b8ced6b",
   "metadata": {},
   "source": [
    "# Learning agenda of this notebook\n",
    "**Advanced Preprocessing**\n",
    "- Stemming\n",
    "- Lemmatization\n",
    "- POS tagging\n",
    "- NER\n",
    "- Parsing\n",
    "- Coreference Resolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3b5f1e7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c070ed94",
   "metadata": {},
   "source": [
    "# Download and Install Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89c00384",
   "metadata": {},
   "outputs": [],
   "source": [
    "!{sys.executable} -m pip install -q --upgrade pip\n",
    "!{sys.executable} -m pip install -q --upgrade numpy pandas sklearn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "94d075b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sys\n",
    "# !pip install gensim wordcloud textblob contractions clean-text unicode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc38c31a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79ba6784",
   "metadata": {},
   "source": [
    "# 1. Stemming\n",
    "\n",
    "- **Word normalization:**\n",
    "\t- Case Folding: Converts all text to lowercase to standardize it.\n",
    "\t- Stemming: Reduces words to their base form by removing prefixes/suffixes.\n",
    "\t- Lemmatization: Similar to stemming but ensures the base form is a valid word by considering the context.\n",
    "\n",
    "\t> **Example:**\n",
    "\t- consult ––> consult\n",
    "\t- consultant ––> consult\n",
    "\t- consultants ––> consult\n",
    "\t- consulting ––> consult\n",
    "\t- consultative ––> consult\n",
    "\t\n",
    "<br/>\n",
    "\n",
    "- By filtering multiple words to their root words, the distinct count of unique words gets reduced without affecting the meaning of the sentence. This has a good effect on the performance of the Machine Learning algorithm.\n",
    "- Both Stemming and Lemmatization aim to reduce terms to their stems and are heavily used in information retrieval.\n",
    "- **Stemming** algorithms use fixed rules such as cutting the prefix/suffix to derive the base/root word. It does so even if the stem itself is not a valid word in the language. It is faster as it cuts words without knowing the context.\n",
    "\n",
    "\t> Example:\n",
    "\t- studies ––> studi\n",
    "\t- cries ––> cri\n",
    "\t\n",
    "<br/>\n",
    "\n",
    "- **Stemming algorithms:**\n",
    "- Porter Stemmer\n",
    "- Snowball Stemmer\n",
    "- Lancaster Stemmer\n",
    "- Regex-based Stemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "731f4d98",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "29fcb284",
   "metadata": {},
   "source": [
    "## a. Stemming using NLTK's `Porter Stemmer`\n",
    "\n",
    "- One of the most common and effective stemming tools is Porter’s Algorithm, developed by Martin Porter in 1980. It is only available for English Language.\n",
    "- Interested students should check out the five phases of word reduction, each with its own set of mapping rules available at the following link:\n",
    "\n",
    "https://tartarus.org/martin/PorterStemmer/def.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "987c2a6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# This paper has been left in plain text form for anyone who wants to copy\n",
    "# the rules out as program comment.\n",
    "\n",
    "# \\...\\ denotes italicisation; {...} denotes superscripting\n",
    "# --------------------------------------------------------------------------\n",
    "\n",
    "#                 An algorithm for suffix stripping\n",
    "\n",
    "#                             M.F.Porter\n",
    "#                                1980\n",
    "\n",
    "# Originally published in \\Program\\, \\14\\ no. 3, pp 130-137, July 1980. (A\n",
    "# few typos have been corrected.)\n",
    "\n",
    "\n",
    "# 1. INTRODUCTION\n",
    "\n",
    "# Removing suffixes by automatic means is an operation which is especially\n",
    "# useful in the field of information retrieval. In a typical IR environment,\n",
    "# one has a collection of documents, each described by the words in the\n",
    "# document title and possibly by words in the document abstract. Ignoring the\n",
    "# issue of precisely where the words originate, we can say that a document is\n",
    "# represented by a vetor of words, or \\terms\\. Terms with a common stem will\n",
    "# usually have similar meanings, for example:\n",
    "\n",
    "#         CONNECT\n",
    "#         CONNECTED\n",
    "#         CONNECTING\n",
    "#         CONNECTION\n",
    "#         CONNECTIONS\n",
    "\n",
    "# Frequently, the performance of an IR system will be improved if term groups\n",
    "# such as this are conflated into a single term. This may be done by removal\n",
    "# of the various suffixes -ED, -ING, -ION, IONS to leave the single term\n",
    "# CONNECT. In addition, the suffix stripping process will reduce the total\n",
    "# number of terms in the IR system, and hence reduce the size and complexity\n",
    "# of the data in the system, which is always advantageous.\n",
    "\n",
    "# Many strategies for suffix stripping have been reported in the\n",
    "# literature.{(e.g. 1-6)} The nature of the task will vary considerably\n",
    "# depending on whether a stem dictionary is being used, whether a suffix\n",
    "# list is being used, and of course on the purpose for which the suffix\n",
    "# stripping is being done. Assuming that one is not making use of a stem\n",
    "# dictionary, and that the purpose of the task is to improve IR performance,\n",
    "# the suffix stripping program will usually be given an explicit list of\n",
    "# suffixes, and, with each suffix, the criterion under which it may be removed\n",
    "# from a word to leave a valid stem. This is the approach adopted here. The\n",
    "# main merits of the present program are that it is small (less than 400 lines\n",
    "# of BCPL), fast (it will process a vocabulary of 10,000 different words in\n",
    "# about 8.1 seconds on the IBM 370/165 at Cambridge University), and\n",
    "# reasonably simple. At any rate, it is simple enough to be described in full\n",
    "# as an algorithm in this paper. (The present version in BCPL is freely\n",
    "# available from the author. BCPL is itself available on a wide range of\n",
    "# different computers, but anyone wishing to use the program should have\n",
    "# little difficulty in coding it up in other programming languages.) Given the\n",
    "# speed of the program, it would be quite realistic to apply it to every word\n",
    "# in a large file of continuous text, although for historical reasons we have\n",
    "# found it convenient to apply it only to relatively small vocabulary lists\n",
    "# derived from continuous text files.\n",
    "\n",
    "# In any suffix stripping program for IR work, two points must be borne in\n",
    "# mind. Firstly, the suffixes are being removed simply to improve IR\n",
    "# performance, and not as a linguistic exercise. This means that it would not\n",
    "# be at all obvious under what circumstances a suffix should be removed, even\n",
    "# if we could exactly determine the suffixes of a word by automatic means.\n",
    "\n",
    "# Perhaps the best criterion for removing suffixes from two words W1 and W2 to\n",
    "# produce a single stem S, is to say that we do so if there appears to be no\n",
    "# difference between the two statements `a document is about W1' and `a\n",
    "# document is about W2'. So if W1=`CONNECTION' and W2=`CONNECTIONS' it seems\n",
    "# very reasonable to conflate them to a single stem. But if W1=`RELATE' and\n",
    "# W2=`RELATIVITY' it seems perhaps unreasonable, especially if the document\n",
    "# collection is concerned with theoretical physics. (It should perhaps be\n",
    "# added that RELATE and RELATIVITY \\are\\ conflated together in the algorithm\n",
    "# described here.) Between these two extremes there is a continuum of\n",
    "# different cases, and given two terms W1 and W2, there will be some variation\n",
    "# in opinion as to whether they should be conflated, just as there is with\n",
    "# deciding the relevance of some document to a query. The evaluation of the\n",
    "# worth of a suffix stripping system is correspondingly difficult.\n",
    "\n",
    "# The second point is that with the approach adopted here, i.e. the use of a\n",
    "# suffix list with various rules, the success rate for the suffix stripping\n",
    "# will be significantly less than 100% irrespective of how the process is\n",
    "# evaluated. For example, if SAND and SANDER get conflated, so most probably\n",
    "# will WAND and WANDER. The error here is that the -ER of WANDER has been\n",
    "# treated as a suffix when in fact it is part of the stem. Equally, a suffix\n",
    "# may completely alter the meaning of a word, in which case its removal is\n",
    "# unhelpful. PROBE and PROBATE for example, have quite distinct meanings in\n",
    "# modern English. (In fact these would not be conflated in our present\n",
    "# algorithm.) There comes a stage in the development of a suffix stripping\n",
    "# program where the addition of more rules to increase the performance in one\n",
    "# area of the vocabulary causes an equal degradation of performance elsewhere.\n",
    "# Unless this phenomenon is noticed in time, it is very easy for the program\n",
    "# to become much more complex than is really necessary. It is also easy to\n",
    "# give undue emphasis to cases which appear to be important, but which turn ut\n",
    "# to be rather rare. For example, cases in which the root of a word changes\n",
    "# with the addition of a suffix, as in DECEIVE/DECEPTION, RESUME/RESUMPTION,\n",
    "# INDEX/INDICES occur much more rarely in real vocabularies than one might at\n",
    "# first suppose. In view of the error rate that must in any case be expected,\n",
    "# it did not seem worthwhile to try and cope with these cases.\n",
    "\n",
    "# It is not obvious that the simplicity of the present program is any demerit.\n",
    "# In a test on the well-known Cranfield 200 collection{7} it gave an\n",
    "# improvement in retrieval performance when compared with a very much more\n",
    "# elaborate program which has been in use in IR research in Cambridge since\n",
    "# 1971{(2,6)}. The test was done as follows: the words of the titles and\n",
    "# abstracts in the documents were passed through the earlier suffix stripping\n",
    "# system, and the resultis stems were used to index the documents. The words\n",
    "# of the queries were reduced to stems in the same way, and the documents were\n",
    "# ranked for each query using term coordination matching of query against\n",
    "# document. From these rankings, recall and precision values were obtained\n",
    "# using the standard recall cutoff method. The entire process was then\n",
    "# repeated using the suffix stripping system described in this paper, and the\n",
    "# results were as follows:\n",
    "\n",
    "#         earlier system        present system\n",
    "#         --------------        --------------\n",
    "#       precision  recall     precision  recall\n",
    "#            0      57.24          0      58.60\n",
    "#           10      56.85         10      58.13\n",
    "#           20      52.85         20      53.92\n",
    "#           30      42.61         30      43.51\n",
    "#           40      42.20         40      39.39\n",
    "#           50      39.06         50      38.85\n",
    "#           60      32.86         60      33.18\n",
    "#           70      31.64         70      31.19\n",
    "#           80      27.15         80      27.52\n",
    "#           90      24.59         90      25.85\n",
    "#          100      24.59        100      25.85\n",
    "\n",
    "# Cleary, the performance is not very different. The important point is that\n",
    "# the earlier, more elaborate system certainly performs no better than the\n",
    "# present, simple system.\n",
    "\n",
    "# (This test was done by prof. C.J. van Rijsbergen.)\n",
    "\n",
    "# 2. THE ALGORITHM\n",
    "\n",
    "# To present the suffix stripping algorithm in its entirety we will need a few\n",
    "# difinitions.\n",
    "\n",
    "# A \\consonant\\ in a word is a letter other than A, E, I, O or U, and other\n",
    "# than Y preceded by a consonant. (The fact that the term `consonant' is\n",
    "# defined to some extent in terms of itself does not make it ambiguous.) So in\n",
    "# TOY the consonants are T and Y, and in SYZYGY they are S, Z and G. If a\n",
    "# letter is not a consonant it is a \\vowel\\.\n",
    "\n",
    "# A consonant will be denoted by c, a vowel by v. A list ccc... of length\n",
    "# greater than 0 will be denoted by C, and a list vvv... of length greater\n",
    "# than 0 will be denoted by V. Any word, or part of a word, therefore has one\n",
    "# of the four forms:\n",
    "\n",
    "#     CVCV ... C\n",
    "#     CVCV ... V\n",
    "#     VCVC ... C\n",
    "#     VCVC ... V\n",
    "\n",
    "# These may all be represented by the single form\n",
    "\n",
    "#     [C]VCVC ... [V]\n",
    "\n",
    "# where the square brackets denote arbitrary presence of their contents.\n",
    "# Using (VC){m} to denote VC repeated m times, this may again be written as\n",
    "\n",
    "#     [C](VC){m}[V].\n",
    "\n",
    "# m will be called the \\measure\\ of any word or word part when represented in\n",
    "# this form. The case m = 0 covers the null word. Here are some examples:\n",
    "\n",
    "#     m=0    TR,  EE,  TREE,  Y,  BY.\n",
    "#     m=1    TROUBLE,  OATS,  TREES,  IVY.\n",
    "#     m=2    TROUBLES,  PRIVATE,  OATEN,  ORRERY.\n",
    "\n",
    "# The \\rules\\ for removing a suffix will be given in the form\n",
    "\n",
    "#     (condition) S1 -> S2\n",
    "\n",
    "# This means that if a word ends with the suffix S1, and the stem before S1\n",
    "# satisfies the given condition, S1 is replaced by S2. The condition is\n",
    "# usually given in terms of m, e.g.\n",
    "\n",
    "#     (m > 1) EMENT ->\n",
    "\n",
    "# Here S1 is `EMENT' and S2 is null. This would map REPLACEMENT to REPLAC,\n",
    "# since REPLAC is a word part for which m = 2.\n",
    "\n",
    "# The `condition' part may also contain the following:\n",
    "\n",
    "# *S  - the stem ends with S (and similarly for the other letters).\n",
    "\n",
    "# *v* - the stem contains a vowel.\n",
    "\n",
    "# *d  - the stem ends with a double consonant (e.g. -TT, -SS).\n",
    "\n",
    "# *o  - the stem ends cvc, where the second c is not W, X or Y (e.g.\n",
    "#        -WIL, -HOP).\n",
    "\n",
    "# And the condition part may also contain expressions with \\and\\, \\or\\ and\n",
    "# \\not\\, so that\n",
    "\n",
    "#     (m>1 and (*S or *T))\n",
    "\n",
    "# tests for a stem with m>1 ending in S or T, while\n",
    "\n",
    "#     (*d and not (*L or *S or *Z))\n",
    "\n",
    "# tests for a stem ending witha double consonant other than L, S or Z.\n",
    "# Elaborate conditions like this are required only rarely.\n",
    "\n",
    "# In a set of rules written beneath each other, only one is obeyed, and this\n",
    "# will be the one with the longest matching S1 for the given word. For\n",
    "# example, with\n",
    "\n",
    "#     SSES -> SS\n",
    "#     IES  -> I\n",
    "#     SS   -> SS\n",
    "#     S    ->\n",
    "\n",
    "# (here the conditions are all null) CARESSES maps to CARESS since SSES is\n",
    "# the longest match for S1. Equally CARESS maps to CARESS (S1=`SS') and CARES\n",
    "# to CARE (S1=`S').\n",
    "\n",
    "# In the rules below, examples of their application, successful or otherwise,\n",
    "# are given on the right in lower case. The algorithm now follows:\n",
    "\n",
    "# Step 1a\n",
    "\n",
    "#     SSES -> SS                         caresses  ->  caress\n",
    "#     IES  -> I                          ponies    ->  poni\n",
    "#                                        ties      ->  ti\n",
    "#     SS   -> SS                         caress    ->  caress\n",
    "#     S    ->                            cats      ->  cat\n",
    "\n",
    "# Step 1b\n",
    "\n",
    "#     (m>0) EED -> EE                    feed      ->  feed\n",
    "#                                        agreed    ->  agree\n",
    "#     (*v*) ED  ->                       plastered ->  plaster\n",
    "#                                        bled      ->  bled\n",
    "#     (*v*) ING ->                       motoring  ->  motor\n",
    "#                                        sing      ->  sing\n",
    "\n",
    "# If the second or third of the rules in Step 1b is successful, the following\n",
    "# is done:\n",
    "\n",
    "#     AT -> ATE                       conflat(ed)  ->  conflate\n",
    "#     BL -> BLE                       troubl(ed)   ->  trouble\n",
    "#     IZ -> IZE                       siz(ed)      ->  size\n",
    "#     (*d and not (*L or *S or *Z))\n",
    "#        -> single letter\n",
    "#                                     hopp(ing)    ->  hop\n",
    "#                                     tann(ed)     ->  tan\n",
    "#                                     fall(ing)    ->  fall\n",
    "#                                     hiss(ing)    ->  hiss\n",
    "#                                     fizz(ed)     ->  fizz\n",
    "#     (m=1 and *o) -> E               fail(ing)    ->  fail\n",
    "#                                     fil(ing)     ->  file\n",
    "\n",
    "# The rule to map to a single letter causes the removal of one of the double\n",
    "# letter pair. The -E is put back on -AT, -BL and -IZ, so that the suffixes\n",
    "# -ATE, -BLE and -IZE can be recognised later. This E may be removed in step\n",
    "# 4.\n",
    "\n",
    "# Step 1c\n",
    "\n",
    "#     (*v*) Y -> I                    happy        ->  happi\n",
    "#                                     sky          ->  sky\n",
    "\n",
    "# Step 1 deals with plurals and past participles. The subsequent steps are\n",
    "# much more straightforward.\n",
    "\n",
    "# Step 2\n",
    "\n",
    "#     (m>0) ATIONAL ->  ATE           relational     ->  relate\n",
    "#     (m>0) TIONAL  ->  TION          conditional    ->  condition\n",
    "#                                     rational       ->  rational\n",
    "#     (m>0) ENCI    ->  ENCE          valenci        ->  valence\n",
    "#     (m>0) ANCI    ->  ANCE          hesitanci      ->  hesitance\n",
    "#     (m>0) IZER    ->  IZE           digitizer      ->  digitize\n",
    "#     (m>0) ABLI    ->  ABLE          conformabli    ->  conformable\n",
    "#     (m>0) ALLI    ->  AL            radicalli      ->  radical\n",
    "#     (m>0) ENTLI   ->  ENT           differentli    ->  different\n",
    "#     (m>0) ELI     ->  E             vileli        - >  vile\n",
    "#     (m>0) OUSLI   ->  OUS           analogousli    ->  analogous\n",
    "#     (m>0) IZATION ->  IZE           vietnamization ->  vietnamize\n",
    "#     (m>0) ATION   ->  ATE           predication    ->  predicate\n",
    "#     (m>0) ATOR    ->  ATE           operator       ->  operate\n",
    "#     (m>0) ALISM   ->  AL            feudalism      ->  feudal\n",
    "#     (m>0) IVENESS ->  IVE           decisiveness   ->  decisive\n",
    "#     (m>0) FULNESS ->  FUL           hopefulness    ->  hopeful\n",
    "#     (m>0) OUSNESS ->  OUS           callousness    ->  callous\n",
    "#     (m>0) ALITI   ->  AL            formaliti      ->  formal\n",
    "#     (m>0) IVITI   ->  IVE           sensitiviti    ->  sensitive\n",
    "#     (m>0) BILITI  ->  BLE           sensibiliti    ->  sensible\n",
    "\n",
    "# The test for the string S1 can be made fast by doing a program switch on\n",
    "# the penultimate letter of the word being tested. This gives a fairly even\n",
    "# breakdown of the possible values of the string S1. It will be seen in fact\n",
    "# that the S1-strings in step 2 are presented here in the alphabetical order\n",
    "# of their penultimate letter. Similar techniques may be applied in the other\n",
    "# steps.\n",
    "\n",
    "# Step 3\n",
    "\n",
    "#     (m>0) ICATE ->  IC              triplicate     ->  triplic\n",
    "#     (m>0) ATIVE ->                  formative      ->  form\n",
    "#     (m>0) ALIZE ->  AL              formalize      ->  formal\n",
    "#     (m>0) ICITI ->  IC              electriciti    ->  electric\n",
    "#     (m>0) ICAL  ->  IC              electrical     ->  electric\n",
    "#     (m>0) FUL   ->                  hopeful        ->  hope\n",
    "#     (m>0) NESS  ->                  goodness       ->  good\n",
    "\n",
    "# Step 4\n",
    "\n",
    "#     (m>1) AL    ->                  revival        ->  reviv\n",
    "#     (m>1) ANCE  ->                  allowance      ->  allow\n",
    "#     (m>1) ENCE  ->                  inference      ->  infer\n",
    "#     (m>1) ER    ->                  airliner       ->  airlin\n",
    "#     (m>1) IC    ->                  gyroscopic     ->  gyroscop\n",
    "#     (m>1) ABLE  ->                  adjustable     ->  adjust\n",
    "#     (m>1) IBLE  ->                  defensible     ->  defens\n",
    "#     (m>1) ANT   ->                  irritant       ->  irrit\n",
    "#     (m>1) EMENT ->                  replacement    ->  replac\n",
    "#     (m>1) MENT  ->                  adjustment     ->  adjust\n",
    "#     (m>1) ENT   ->                  dependent      ->  depend\n",
    "#     (m>1 and (*S or *T)) ION ->     adoption       ->  adopt\n",
    "#     (m>1) OU    ->                  homologou      ->  homolog\n",
    "#     (m>1) ISM   ->                  communism      ->  commun\n",
    "#     (m>1) ATE   ->                  activate       ->  activ\n",
    "#     (m>1) ITI   ->                  angulariti     ->  angular\n",
    "#     (m>1) OUS   ->                  homologous     ->  homolog\n",
    "#     (m>1) IVE   ->                  effective      ->  effect\n",
    "#     (m>1) IZE   ->                  bowdlerize     ->  bowdler\n",
    "\n",
    "# The suffixes are now removed. All that remains is a little tidying up.\n",
    "\n",
    "# Step 5a\n",
    "\n",
    "#     (m>1) E     ->                  probate        ->  probat\n",
    "#                                     rate           ->  rate\n",
    "#     (m=1 and not *o) E ->           cease          ->  ceas\n",
    "\n",
    "# Step 5b\n",
    "\n",
    "#     (m > 1 and *d and *L) -> single letter\n",
    "#                                     controll       ->  control\n",
    "#                                     roll           ->  roll\n",
    "\n",
    "# The algorithm is careful not to remove a suffix when the stem is too short,\n",
    "# the length of the stem being given by its measure, m. There is no linguistic\n",
    "# basis for this approach. It was merely observed that m could be used quite\n",
    "# effectively to help decide whether or not it was wise to take off a suffix.\n",
    "# For example, in the following two lists:\n",
    "\n",
    "#                   list A        list B\n",
    "#                   ------        ------\n",
    "#                   RELATE        DERIVATE\n",
    "#                   PROBATE       ACTIVATE\n",
    "#                   CONFLATE      DEMONSTRATE\n",
    "#                   PIRATE        NECESSITATE\n",
    "#                   PRELATE       RENOVATE\n",
    "\n",
    "# -ATE is removed from the list B words, but not from the list A words. This\n",
    "# means that the pairs DERIVATE/DERIVE, ACTIVATE/ACTIVE, DEMONSTRATE/DEMONS-\n",
    "# TRABLE, NECESSITATE/NECESSITOUS, will conflate together. The fact that no\n",
    "# attempt is made to identify prefixes can make the results look rather\n",
    "# inconsistent. Thus PRELATE does not lose the -ATE, but ARCHPRELATE becomes\n",
    "# ARCHPREL. In practice this does not matter too much, because the presence of\n",
    "# the prefix decreases the probability of an erroneous conflation.\n",
    "\n",
    "# Complex suffixes are removed bit by bit in the different steps. Thus\n",
    "# GENERALIZATIONS is stripped to GENERALIZATION (Step 1), then to GENERALIZE\n",
    "# (Step 2), then to GENERAL (Step 3), and then to GENER (Step 4). OSCILLATORS\n",
    "# is stripped to OSCILLATOR (Step 1), then to OSCILLATE (Step 2), then to\n",
    "# OSCILL (Step 4), and then to OSCIL (Step 5). In a vocabulary of 10,000\n",
    "# words, the reduction in size of the stem was distributed among the steps as\n",
    "# follows:\n",
    "\n",
    "#     Suffix stripping of a vocabulary of 10,000 words\n",
    "#     ------------------------------------------------\n",
    "#     Number of words reduced in step 1:   3597\n",
    "#                   \"                 2:    766\n",
    "#                   \"                 3:    327\n",
    "#                   \"                 4:   2424\n",
    "#                   \"                 5:   1373\n",
    "#     Number of words not reduced:         3650\n",
    "\n",
    "# The resulting vocabulary of stems contained 6370 distinct entries. Thus the\n",
    "# suffix stripping process reduced the size of the vocabulary by about one\n",
    "# third.\n",
    "\n",
    "# REFERENCIES\n",
    "\n",
    "# 1.  LOVINS, J.B. Development of a Stemming Algorithm. \\Mechanical\n",
    "#     Translation and computation Linguistics\\. \\11\\ (1) March 1968 pp 23-31.\n",
    "\n",
    "# 2.  ANDREWS, K. The Development of a Fast Conflation Algorithm for English.\n",
    "#     \\Dissertation for the Diploma in Computer Science\\, Computer\n",
    "#     Laboratory, University of Cambridge, 1971.\n",
    "\n",
    "# 3.  PETRARCA, A.E. and LAY W.M. Use of an automatically generated authority\n",
    "#     list to eliminate scattering caused by some singular and plural main\n",
    "#     index terms. \\Proceedings of the American Society for Information\n",
    "#     Science\\, \\6\\ 1969 pp 277-282.\n",
    "\n",
    "# 4.  DATTOLA, Robert T. \\FIRST: Flexible Information Retrieval System for\n",
    "#     Text\\. Webster N.Y: Xerox Corporation, 12 Dec 1975.\n",
    "\n",
    "# 5.  COLOMBO, D.S. and NIEHOFF R.T. \\Final report on improved access to\n",
    "#     scientific and technical information through automated vocabulary\n",
    "#     switching.\\ NSF Grant No. SIS75-12924 to the National Science\n",
    "#     Foundation.\n",
    "\n",
    "# 6.  DAWSON, J.L. Suffix Removal and Word Conflation. \\ALLC Bulletin\\,\n",
    "#     Michaelmas 1974 p.33-46.\n",
    "\n",
    "# 7.  CLEVERDON, C.W., MILLS J. and KEEN M. \\Factors Determining the\n",
    "#     Performance of Indexing Systems\\ 2 vols. College of Aeronautics,\n",
    "#     Cranfield 1966."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1dc19c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3827cbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the toolkit and the PorterStemmer() method from the stem module\n",
    "import nltk \n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem.regexp import RegexpStemmer\n",
    "\n",
    "ps = PorterStemmer()\n",
    "dir(ps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b93101cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "ps._step1a(\"dogs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f2c9156",
   "metadata": {},
   "outputs": [],
   "source": [
    "ps._step1a(\"studies\"), ps._step1a(\"studying\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95030a60",
   "metadata": {},
   "outputs": [],
   "source": [
    "ps._step1b(\"motoring\"), ps._step1b(\"studying\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89dc75e9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03aaf29f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd7689fe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e1b43b6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7ca3bc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "mystr = \"wait waiting waited waits \\\n",
    "consult consultant consultants consulting consultative \\\n",
    "university universal universe \\\n",
    "studies cries cry bicycle USA \\\n",
    "data datum\"\n",
    "\n",
    "# Create the PorterStemmer object\n",
    "ps = PorterStemmer()\n",
    "\n",
    "# Use a loop and get the stem word by passing a word to stem() method\n",
    "for word in nltk.word_tokenize(mystr):\n",
    "    root_word = ps.stem(word)\n",
    "    print(word, \"---->\", root_word)\n",
    "\n",
    "# You can use list comprehension instead of loop\n",
    "# [ps.stem(word) for word in nltk.word_tokenize(mystr)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e10e25c",
   "metadata": {},
   "source": [
    "\n",
    "**Over-Stemming:** Over-stemming occurs when a stemming algorithm is too aggressive and removes more characters than necessary, leading to a loss of meaningful distinctions between words. This results in unrelated words being reduced to the same root, which can misrepresent the context.\n",
    "- **Words:** universe and universal\n",
    "- **Stemmed Result:** Both are reduced to univers\n",
    "\n",
    "\n",
    "**Under-Stemming:** Under-stemming happens when a stemming algorithm is too conservative and fails to reduce related words to the same root. This results in missed opportunities to group words that have the same meaning or intent.\n",
    "- **Words:** waiting and waited\n",
    "- **Stemmed Result:** wait (correctly reduced)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d00ddc5b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a52015e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3ca00acb",
   "metadata": {},
   "source": [
    "# 2. Lemmatization\n",
    "\n",
    "- **Stemming algorithms** use fixed rules such as cutting the prefix/suffix to derive the base/root word. It does so even if the stem itself is not a valid word in the language. It is faster as it cuts words without knowing the context.\n",
    "\n",
    "- **Lemmatization** uses knowledge of a language (a.k.a. linguistic knowledge) to derive the base/root word, also known as the lemma. Lemmatization ensures that the root word (lemma) belongs to a language. Since lemmatization involves deriving the meaning of a word from something like a dictionary, it’s very time-consuming.\n",
    "\n",
    "<br>\n",
    "\n",
    "| **Feature**                                                                                       | **Stemming**                                                                                       | **Lemmatization**                                                                                 |\n",
    "|---------------------------------------------------------------------------------------------------|-----------------------------------------------------------------------------------------------------|---------------------------------------------------------------------------------------------------|\n",
    "| **Speed**                                                                                        | Stemming is faster because it chops words without knowing the context of the word in given sentences. | Lemmatization is slower as it considers the context of the word before proceeding.                |\n",
    "| **Approach**                                                                                     | It is a rule-based approach.                                                                       | It is a dictionary-based approach.                                                              |\n",
    "| **Accuracy**                                                                                     | Accuracy is less.                                                                                  | Accuracy is more as compared to stemming.                                                       |\n",
    "| **Meaning Preservation**                                                                         | Stemming may create a non-existent meaning of a word.                                              | Lemmatization always gives the dictionary word while converting into root form.                 |\n",
    "| **Use Case**                                                                                     | Stemming is preferred when the meaning of the word is not important for analysis. Example: Spam Detection | Lemmatization is preferred when the meaning of the word is important for analysis. Example: Question Answer |\n",
    "| **Example**                                                                                      | `Studies` → `Studi`                                                                                | `Studies` → `Study`                                                                              |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "796a1db9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89c90242",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6705d6b7",
   "metadata": {},
   "source": [
    "## a. Lemmatization Using NLTK’s WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08719058",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b64ad309",
   "metadata": {},
   "outputs": [],
   "source": [
    "dir(lemmatizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a267b76",
   "metadata": {},
   "outputs": [],
   "source": [
    "mystr = \"university universal universe \\\n",
    "studies cries cry bicycle USA \\\n",
    "data datum\"\n",
    "\n",
    "for word in nltk.word_tokenize(mystr):\n",
    "    print(word, \"---->\", lemmatizer.lemmatize(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e11a15ca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4909252b",
   "metadata": {},
   "source": [
    "# 3. Overview of Part of Speech (POS)\n",
    "\n",
    "## a. What are English Parts of Speech (POS)?\n",
    "\n",
    "<h3 align=\"center\"><div class=\"alert alert-success\" style=\"margin: 20px\">Ehtisham ate fruits</h3>\n",
    " \n",
    "\n",
    "- Every sentence is made up of different parts/words, and those parts are called **parts of speech**.\n",
    "- According to Wikipedia, there are roughly ten parts of speech in the English language.\n",
    "\n",
    "### Example:\n",
    "- In this sentence:\n",
    "  - \"Ehtisham\" represents a **noun** (person).\n",
    "  - \"ate\" represents a **verb** (action).\n",
    "  - \"fruits\" represents a **noun** (object)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb9c503b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fd5bbe20-83e9-4691-8eba-28bc69798fb6",
   "metadata": {},
   "source": [
    "<h3 align=\"center\"><div class=\"alert alert-success\" style=\"margin: 20px\">The quick brown fox name is Zoro and it high jumped over the lazy dog’s back.</h3>\n",
    "\n",
    "1. **Noun and Proper Noun**  \n",
    "   - Example: \"fox\", \"name\", \"Zoro\", \"dog\"\n",
    "   \n",
    "2. **Pronoun**  \n",
    "   - Example: \"it\"\n",
    "\n",
    "3. **Adjective**  \n",
    "   - Example: \"quick\", \"brown\", \"lazy\"\n",
    "\n",
    "4. **Verb**  \n",
    "   - Example: \"is\", \"jumped\"\n",
    "\n",
    "5. **Adverb**  \n",
    "   - Example: \"high\"\n",
    "\n",
    "6. **Conjunctions**  \n",
    "   - Example: \"and\"\n",
    "\n",
    "7. **Determiner/Article**  \n",
    "   - Example: \"The\", \"the\"\n",
    "\n",
    "8. **Prepositions**  \n",
    "   - Example: \"over\"\n",
    "\n",
    "9. **Interjections**  \n",
    "   - (None in this sentence)\n",
    "\n",
    "10. **Numeral**  \n",
    "   - (None in this sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b46183c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# POS is a kind of classification problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f3546a8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a06425e7",
   "metadata": {},
   "source": [
    "## b. What, Why and How to do POS Tagging?\n",
    "\n",
    "### (i) What?\n",
    "- **Definition:** POS tagging is the process of assigning a part-of-speech to each word in a text.\n",
    "\n",
    "\n",
    "**Categories of POS**:\n",
    "1. **Closed Class Type**\n",
    "   - Includes: Prepositions, pronouns, determiners.\n",
    "2. **Open Class Type**\n",
    "   - Includes: Nouns, verbs, adjectives, adverbs.\n",
    "\n",
    "\n",
    "<h3 align=\"center\"><div class=\"alert alert-success\" style=\"margin: 20px\">Ehtisham is playing cricket.</h3>\n",
    "\n",
    "#### POS Tags for Each Word:\n",
    "- **Ehtisham:** Proper Noun (PROPN)\n",
    "- **is:** Auxiliary Verb (AUX)\n",
    "- **playing:** Verb (VERB)\n",
    "- **cricket:** Noun (NOUN)\n",
    "\n",
    "#### Dependency Parsing:\n",
    "- **Ehtisham:** Subject (nsubj)\n",
    "- **is:** Auxiliary Verb (aux)\n",
    "- **playing:** Main Verb\n",
    "- **cricket:** Direct Object (dobj)\n",
    "\n",
    "\n",
    "\n",
    "### (ii) Why? \n",
    "#### Applications of POS Tagging:\n",
    "- Named Entity Recognition (Information Retrieval).\n",
    "- Coreference Resolution.\n",
    "- Word Sense Disambiguation.\n",
    "- Sentence Parsing.\n",
    "- Question Answering Systems and Chatbots.\n",
    "\n",
    "\n",
    "\n",
    "## (iii) How?\n",
    "1. **Rule-Based POS Tagging:**\n",
    "   - Example: E-Brill's Tagger.\n",
    "2. **Stochastic POS Tagging:**\n",
    "   - Example: Hidden Markov Model (HMM) and Viterbi Algorithm.\n",
    "3. **Deep Learning Approaches:**\n",
    "   - Contextual tagging using neural networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4da2105",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "702a7d17-e290-43ab-93d6-7967b6328f21",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "305dcaf2",
   "metadata": {},
   "source": [
    "## c. POS Tagging using `spaCy`\n",
    "\n",
    "## Overview\n",
    "- **POS Tagging in spaCy**:\n",
    "  - Simple and efficient process.\n",
    "  - Instantiate a spaCy object as `doc`.\n",
    "  - Iterate over tokens of the spaCy `Doc` object and use `pos_` and `tag_` to print coarse-grained and fined-grained POS tag.\n",
    "  - Use `spacy.explain()` to get detailed explanations of POS tags.\n",
    "\n",
    "- **For More Details**:\n",
    "  - Visit [spaCy Linguistic Features Documentation](https://spacy.io/usage/linguistic-features).\n",
    "\n",
    "\n",
    "**Example 1:**\n",
    "\n",
    "<h3 align=\"center\"><div class=\"alert alert-success\" style=\"margin: 20px\">The quick brown fox name is Zoro and it high jumped over the lazy dog’s back.</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0eeb1d3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the english library of spacy\n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a31ccf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a pre-trained model, let's check the components of the pipeline\n",
    "nlp.pipe_names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36cf54d1",
   "metadata": {},
   "source": [
    "<img src=\"images/spacy-doc.png\">\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### Components of the Pipeline:\n",
    "1. **Input:** `Text`\n",
    "2. **Processing Steps:**\n",
    "   - **tok2vec:** Converts tokens to vectors for further processing.\n",
    "   - **tagger:** Assigns Part of Speech (POS) tags to tokens.\n",
    "   - **parser:** Analyzes the syntactic structure of the sentence.\n",
    "   - **attribute_ruler:** Applies rules to set linguistic attributes.\n",
    "   - **lemmatizer:** Converts tokens to their base forms (lemmas).\n",
    "   - **ner (Named Entity Recognizer):** Identifies and labels named entities.\n",
    "3. **Output:** `Doc` object, containing all processed information.\n",
    "\n",
    "### Key Points:\n",
    "- A **pipe** is an individual component of the pipeline, responsible for specific tasks.\n",
    "- **Pipeline Workflow:**\n",
    "  - The **tokenizer** splits the text into individual tokens.\n",
    "  - The **parser** structures the text syntactically.\n",
    "  - The **NER** module identifies entities and assigns appropriate labels.\n",
    "- The **Doc** object stores all the processed data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59760bd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "mytext = \"The quick brown fox name is Zoro and it high jumped over the lazy dog's back.\"\n",
    "doc = nlp(mytext)\n",
    "type(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "267bbe8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try printing the object:\n",
    "doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ff76346",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Document level attributes\n",
    "print(dir(doc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f73f5c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fd9bf33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Token attributes\n",
    "print(doc[1])  # Example token\n",
    "# dir(doc[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0af3944",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a27af20d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\033[1m')  # Bold formatting for the header\n",
    "print(f'{\"Token\":{10}} {\"Coarse POS Tag\":{15}} {\"Fine-Grained POS Tag\":{21}} {\"Explanation\"}')\n",
    "print('\\033[m')  # Reset formatting\n",
    "for token in doc:\n",
    "    print(f'{token.text:{10}} {token.pos_:{15}} {token.tag_:{21}} {spacy.explain(token.tag_)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eba13d6",
   "metadata": {},
   "source": [
    "**Output**\n",
    "- This code iterates through each token in the doc object and prints:\n",
    "- The token’s text\n",
    "- Its coarse-grained POS tag\n",
    "- Its fine-grained POS tag\n",
    "- A brief explanation of the fine-grained POS tag"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e07047fb",
   "metadata": {},
   "source": [
    "### Additional Token Attributes in spaCy\n",
    "\n",
    "The `spaCy` `Doc` object provides many associated attributes for each token, offering detailed linguistic information. Below are some of the key token attributes and their descriptions:\n",
    "\n",
    "| **Tag**       | **Description**                                                  |\n",
    "|---------------|------------------------------------------------------------------|\n",
    "| `.text`       | The original word text                                           |\n",
    "| `.lemma_`     | The base form of the word                                        |\n",
    "| `.pos_`       | The simple part-of-speech tag                                    |\n",
    "| `.tag_`       | The detailed part-of-speech tag                                  |\n",
    "| `.shape_`     | The word shape – capitalization, punctuation, digits            |\n",
    "| `.is_alpha`   | Token text consists of alphanumeric characters                  |\n",
    "| `.is_ascii`   | Token text consists of ASCII characters                         |\n",
    "| `.is_digit`   | Token text consists of digits                                   |\n",
    "| `.is_lower`   | Token text is in lowercase                                      |\n",
    "| `.is_upper`   | Token text is in uppercase                                      |\n",
    "| `.is_title`   | Token text is in titlecase                                      |\n",
    "| `.is_punct`   | Token is punctuation                                            |\n",
    "| `.is_space`   | Token is whitespace                                             |\n",
    "| `.is_stop`    | Token is a stopword (e.g., common words like \"the\", \"is\")       |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75e9a172",
   "metadata": {},
   "source": [
    "**Coarse-Grained POS Tags**: Spacy assigns a coarse-grained part-of-speech tag to each token.\n",
    "\n",
    "| **POS**   | **Description**         | **Example**         |\n",
    "|-----------|-------------------------|---------------------|\n",
    "| NOUN      | Noun                    | dog, car, happiness |\n",
    "| VERB      | Verb                    | run, eat, sing      |\n",
    "| ADJ       | Adjective               | happy, red, tall    |\n",
    "| ADV       | Adverb                  | quickly, very, well |\n",
    "| PRON      | Pronoun                 | he, she, they       |\n",
    "| DET       | Determiner              | the, a, some        |\n",
    "| ADP       | Adposition (preposition)| in, on, at          |\n",
    "| CONJ      | Conjunction             | and, but, or        |\n",
    "| NUM       | Numeral                 | one, two, three     |\n",
    "| PUNCT     | Punctuation             | ., ?, !             |\n",
    "| INTJ      | Interjection            | oh, wow, oops       |\n",
    "| PART      | Particle                | not, to (as in to go)|"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63c4d811",
   "metadata": {},
   "source": [
    "**Fine-Grained POS Tags**: Spacy also provides fine-grained part-of-speech tags.\n",
    "\n",
    "| **Coarse-Grained POS Tag** | **Coarse-Grained Description** | **Fine-Grained Tag** | **Fine-Grained Description**        | **Morphology**                     |\n",
    "|----------------------------|--------------------------------|----------------------|-------------------------------------|-------------------------------------|\n",
    "| NOUN                       | Noun                          | NN                   | Singular noun                      | Number=Sing                        |\n",
    "| NOUN                       | Noun                          | NNS                  | Plural noun                        | Number=Plur                        |\n",
    "| VERB                       | Verb                          | VB                   | Base form                          | VerbForm=Inf                       |\n",
    "| VERB                       | Verb                          | VBD                  | Past tense verb                    | Tense=Past                         |\n",
    "| VERB                       | Verb                          | VBG                  | Gerund or present participle       | VerbForm=Part; Tense=Pres          |\n",
    "| ADJ                        | Adjective                     | JJ                   | Adjective                          | Degree=Pos                         |\n",
    "| ADJ                        | Adjective                     | JJR                  | Comparative adjective              | Degree=Cmp                         |\n",
    "| ADJ                        | Adjective                     | JJS                  | Superlative adjective              | Degree=Sup                         |\n",
    "| ADV                        | Adverb                        | RB                   | Adverb                             | Degree=Pos                         |\n",
    "| ADV                        | Adverb                        | RBR                  | Comparative adverb                 | Degree=Cmp                         |\n",
    "| ADV                        | Adverb                        | RBS                  | Superlative adverb                 | Degree=Sup                         |\n",
    "| PRON                       | Pronoun                      | PRP                  | Personal pronoun                   | Case=Nom/Acc                       |\n",
    "| PRON                       | Pronoun                      | PRP$                 | Possessive pronoun                 | Case=Gen                           |\n",
    "| DET                        | Determiner                   | DT                   | Determiner                         | Definite=Def; PronType=Art         |\n",
    "| ADP                        | Adposition                   | IN                   | Preposition or subordinating conjunction |                                  |\n",
    "| CONJ                       | Conjunction                  | CC                   | Coordinating conjunction           |                                  |\n",
    "| NUM                        | Numeral                      | CD                   | Cardinal number                    | NumType=Card                       |\n",
    "| PUNCT                      | Punctuation                  | .                    | Sentence-ending punctuation        |                                  |\n",
    "| INTJ                       | Interjection                 | UH                   | Interjection                       |                                  |\n",
    "| PART                       | Particle                     | TO                   | \"to\" as infinitive marker          |                                  |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18073e69",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8b53789d",
   "metadata": {},
   "source": [
    "### Example 2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ca6b4c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp('I am going to make dinner.')\n",
    "word = doc[4]  # Access the word \"make\"\n",
    "print(word, word.pos_, word.tag_, spacy.explain(word.tag_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f506de94",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp('What is the make of your laptop?')\n",
    "word = doc[3]  # Access the word \"make\"\n",
    "print(word, word.pos_, word.tag_, spacy.explain(word.tag_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b148ed59",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f689fa15",
   "metadata": {},
   "source": [
    "**Explanation:**\n",
    "- **word.pos_**: Provides the coarse-grained part-of-speech tag.\n",
    "- **word.tag_**: Provides the fine-grained part-of-speech tag.\n",
    "- **spacy.explain(word.tag_)**: Explains the fine-grained tag in detail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e7bdb8d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "45d6de0f",
   "metadata": {},
   "source": [
    "### Example 3:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cd51572",
   "metadata": {},
   "outputs": [],
   "source": [
    "# doc = nlp('Air currents carried the balloon for miles.')\n",
    "# doc = nlp('Ehtisham is swimming against the current.')\n",
    "doc = nlp('Strong currents pulled the swimmer out to sea')\n",
    "word = doc[1]  # Access the word \"currents\"\n",
    "print(word.text, \"--->\", word.pos_, word.tag_, spacy.explain(word.tag_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# doc = nlp('The current version of this notebook is on cloud.')\n",
    "# doc = nlp('I am satisfied with my current income.')\n",
    "doc = nlp('The current balance is Rs.5425.')\n",
    "word = doc[1]  # Access the word \"current\"\n",
    "print(word.text, \"--->\", word.pos_, word.tag_, spacy.explain(word.tag_))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93576a2c",
   "metadata": {},
   "source": [
    "### Example 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af57e73a",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(u'Ehtisham reads NLP books.')\n",
    "word = doc[1]  # reads\n",
    "print(word, \"---->\", word.pos_, word.tag_, spacy.explain(word.tag_))\n",
    "\n",
    "# Example 2: Past tense\n",
    "doc = nlp(u'Ehtisham read NLP books.')\n",
    "word = doc[1]  # read\n",
    "print(word, \"---->\", word.pos_, word.tag_, spacy.explain(word.tag_))\n",
    "\n",
    "# Example 3: Past participle\n",
    "doc = nlp(u'Ehtisham has read many NLP book.')\n",
    "word = doc[2]  # read\n",
    "print(word, \"---->\", word.pos_, word.tag_, spacy.explain(word.tag_))\n",
    "\n",
    "# Example 4: Gerund or present participle\n",
    "doc = nlp(u'Ehtisham is reading the NLP book.')\n",
    "word = doc[2]  # reading\n",
    "print(word, \"---->\", word.pos_, word.tag_, spacy.explain(word.tag_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5317bb1d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c23c4f5e",
   "metadata": {},
   "source": [
    "### Example: Counting POS Tags\n",
    "- The `Doc.count_by()` method accepts a specific token attribute as its argument and returns a frequency count of the given attribute as a dictionary object. The keys in the dictionary are the integer values of the given attribute ID, and the values are their frequency. Counts of zero are not included.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "998e54b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "mystr = \"I am a runner running in a race because I love to run since I ran today\"\n",
    "doc = nlp(mystr)\n",
    "\n",
    "# Count the frequencies of different coarse-grained POS tags:\n",
    "POS_counts = doc.count_by(spacy.attrs.POS)\n",
    "POS_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a7ff6a9",
   "metadata": {},
   "source": [
    "The output is a dictionary with:\n",
    "- `Keys`: POS numerical codes\n",
    "- `Values`: Count of occurrences of that POS in the raw text string.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "036ad42d",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc[4].pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45fd4bc4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eed7dccf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e6aef533",
   "metadata": {},
   "source": [
    "### Visualize POS using `Spacy`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c2aa032",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy import displacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(\"Ehtisham is playling cricket\")\n",
    "displacy.render(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5784dd3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d6a30dba",
   "metadata": {},
   "source": [
    "**Customizing the Appearance of Token Visualization in spaCy**: When visualizing tokens, you can customize the appearance by passing arguments to the `options` parameter.\n",
    "\n",
    "\n",
    "| **NAME**    | **TYPE**  | **DESCRIPTION**                                                                 | **DEFAULT** |\n",
    "|-------------|-----------|---------------------------------------------------------------------------------|-------------|\n",
    "| `compact`   | `bool`    | \"Compact mode\" with square arrows that takes up less space.                     | `False`     |\n",
    "| `distance`  | `integer` | Distance between tokens.                                                        | `100`       |\n",
    "| `color`     | `unicode` | Text color (HEX, RGB, or color names).                                          | `#000000`   |\n",
    "| `bg`        | `unicode` | Background color (HEX, RGB, or color names).                                    | `#ffffff`   |\n",
    "| `font`      | `unicode` | Font name or font family for all text.                                          | `Arial`     |\n",
    "\n",
    "For more options, visit: [spaCy Display Options Documentation](https://spacy.io/api/top-level#displacy_options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fbe2071",
   "metadata": {},
   "outputs": [],
   "source": [
    "options = {\n",
    "    'compact': 'True',\n",
    "    'distance': 200,\n",
    "    'color': 'white',\n",
    "    'bg': 'black',\n",
    "    'font': 'Arial'\n",
    "}\n",
    "\n",
    "displacy.render(doc, options=options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2da03515",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68f4af1a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3f8e6bcf",
   "metadata": {},
   "source": [
    "# 4. Overview of Named Entity Recognition (NER)\n",
    "\n",
    "- **Entity**: A common thing that belongs to a noun family. It can be a subject or object.  \n",
    "\n",
    "<h5 align=\"center\"><div class=\"alert alert-success\" style=\"margin: 20px\">President will meet the chairman of the company in the capital city.</h5>\n",
    "\n",
    "- **Named Entity**: Refers to a real-world or conceptual object that can be represented by a proper noun.  \n",
    "\n",
    "<h5 align=\"center\"><div class=\"alert alert-success\" style=\"margin: 20px\">Trump will meet the chairman of Google in New York city.</h5>\n",
    "\n",
    "\n",
    "\n",
    "**Examples of Named Entities**\n",
    "\n",
    "| **Type**          | **Examples**                     |\n",
    "|--------------------|----------------------------------|\n",
    "| **Organization**   | PUCIT, NADRA, WHO               |\n",
    "| **Person**         | Ehtisham, Tim                   |\n",
    "| **Location**       | Pakistan, Lahore, Mount Everest |\n",
    "| **Address**        | 131-E Model Town                |\n",
    "| **Date**           | July, 02/09/2022                |\n",
    "| **Time**           | 1:15 pm                         |\n",
    "| **Money**          | Rs.100/-                        |\n",
    "| **Percent**        | 18.75%                          |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "934a7e97",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "793b97fb",
   "metadata": {},
   "source": [
    "## What, Why, and How to do NER?\n",
    "\n",
    "### (i) What is NER?\n",
    "- **Named Entity Recognition (NER)** is a subtask of **Information Extraction** that seeks to locate and classify named entities mentioned in unstructured text into predefined categories such as:\n",
    "  - Person names\n",
    "  - Organizations\n",
    "  - Locations\n",
    "  - Medical codes\n",
    "  - Date-time expressions\n",
    "  - Monetary values\n",
    "  - Percentages, etc.\n",
    "\n",
    "- It is also known as **Entity Identification**, **Entity Chunking**, or **Entity Extraction**.\n",
    "\n",
    "**Example**\n",
    "<h5 align=\"center\"><div class=\"alert alert-success\" style=\"margin: 20px\">Ehtisham has moved to Karachi where he will be playing basketball on 25 December 2022</h5>\n",
    "\n",
    "**NER Output**:  \n",
    "- **Ehtisham** → `PERSON`  \n",
    "- **Karachi** → `GPE`  \n",
    "- **playing** → `EVENT`  \n",
    "- **basketball** → `PRODUCT`  \n",
    "- **25 December 2022** → `DATE`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64b46e8e",
   "metadata": {},
   "source": [
    "**NER Types/Tags:**\n",
    "\n",
    "| **TYPE**      | **DESCRIPTION**                                     | **EXAMPLE**                                 |\n",
    "|---------------|-----------------------------------------------------|---------------------------------------------|\n",
    "| `PERSON`      | People, including fictional                         | *Fred Flintstone*                           |\n",
    "| `NORP`        | Nationalities or religious or political groups      | *The Republican Party*                      |\n",
    "| `FAC`         | Buildings, airports, highways, bridges, etc.        | *Logan International Airport, The Golden Gate* |\n",
    "| `ORG`         | Companies, agencies, institutions, etc.             | *Microsoft, FBI, MIT*                       |\n",
    "| `GPE`         | Geo-Political Entities, countries, cities, states   | *France, UAR, Chicago, Idaho*               |\n",
    "| `LOC`         | Non-GPE locations, mountain ranges, bodies of water | *Europe, Nile River, Midwest*               |\n",
    "| `PRODUCT`     | Objects, vehicles, foods, etc. (Not services)       | *Formula 1*                                 |\n",
    "| `EVENT`       | Named events                                        | *Olympics, World Cup, WWII*                 |\n",
    "| `WORK_OF_ART` | Titles of books, songs, etc.                        | *The Catcher in the Rye, Bohemian Rhapsody* |\n",
    "| `LAW`         | Named documents made into laws                      | *First Amendment*                           |\n",
    "| `LANGUAGE`    | Any named language                                  | *English, French, Mandarin*                 |\n",
    "| `DATE`        | Absolute or relative dates or periods               | *25 December 2022*                          |\n",
    "| `TIME`        | Times smaller than a day                            | *2:15 PM, noon*                             |\n",
    "| `PERCENT`     | Percentage values                                   | *18.75%*                                    |\n",
    "| `MONEY`       | Monetary values, including currency                 | *Rs.100/-*                                  |\n",
    "| `QUANTITY`    | Measurements that are not money                     | *5 kg, 10 miles*                            |\n",
    "| `ORDINAL`     | \"First\", \"second\", etc.                             | *First, Second*                             |\n",
    "| `CARDINAL`    | Numerals that are not ordinal                       | *One, Two, 100*                             |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cf983bb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0f5ddd4f",
   "metadata": {},
   "source": [
    "### (ii) Why NER?\n",
    "\n",
    "- **Coreference Resolution**: Resolving references to entities in a document to the same entity.\n",
    "- **Information Extraction**: Extracting structured data from unstructured text.\n",
    "- **Search Engines**: Improving search by recognizing entities in queries and documents.\n",
    "- **Recommendation Systems**: Suggesting relevant entities like products, locations, or names based on recognized entities.\n",
    "- **Question Answering Systems and Chatbots**: Enhancing understanding and response accuracy by identifying entities in user queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ac43687",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5156e987",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e914ef2c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8e1fffb8",
   "metadata": {},
   "source": [
    "\n",
    "### (iii) How to do NER?\n",
    "\n",
    "##### 1. **Dictionary-Based Approach**\n",
    "- **Definition**: Stores named entities in a list called a gazetteer.\n",
    "- **Limitations**: \n",
    "  - Mainly targets proper nouns or unregistered words.\n",
    "  - Unable to handle new words or context-dependent entities.\n",
    "- **Example**: \n",
    "  - \"Ehtisham was born in 2001.\"  \n",
    "    Here \"Ehtisham\" could be recognized as a `PERSON` entity.\n",
    "- **Implementation in spaCy**: \n",
    "  - spaCy provides a class called `EntityRuler` to define rules.\n",
    "\n",
    "\n",
    "##### 2. **Rule-Based Approach**\n",
    "- **Definition**: Relies on predefined rules and patterns to recognize entities.\n",
    "- **Examples**:\n",
    "  - If a word is followed by \"Inc.\" and starts with a capital letter, it is identified as an `ORGANIZATION`.\n",
    "  - Using regex to identify patterns like phone numbers or dates.\n",
    "- **Advantages**:\n",
    "  - Provides control over entity recognition.\n",
    "  - Works well for structured formats.\n",
    "\n",
    "\n",
    "##### 3. **Machine Learning-Based Approach**\n",
    "- **Definition**: Tags named entities based on context and without relying on predefined lists.\n",
    "- **Techniques Used**:\n",
    "  - **Support Vector Machines (SVMs)**: Classification for entity recognition.\n",
    "  - **Hidden Markov Models (HMMs)**: Sequential data modeling for entity tagging.\n",
    "  - **Conditional Random Fields (CRFs)**: Predicting sequences of labels for inputs.\n",
    "- **Advantages**:\n",
    "  - Can generalize to new entities.\n",
    "  - Context-sensitive.\n",
    "- **Limitation**:\n",
    "  - Requires labeled training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2bfefaf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5120cdcb",
   "metadata": {},
   "source": [
    "## b. Named Entity Recognition using `spaCy`\n",
    "\n",
    "- **spaCy** can recognize various types of named entities in a document by asking the model for a prediction. \n",
    "- Since these models are statistical and depend heavily on the training examples, they might require fine-tuning for specific use cases.\n",
    "- spaCy includes an `ner` pipeline component that identifies token spans fitting a predefined set of named entities. These entities are accessible through the `ents` property of a `Doc` object.\n",
    "\n",
    "For more details, visit the [spaCy Documentation on NER](https://spacy.io/usage/linguistic-features#named-entities).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1f6875e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tok2vec', 'tagger', 'parser', 'attribute_ruler', 'lemmatizer', 'ner']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the English library of spaCy\n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# this is a pre-trained model, let us check the pipes of the pipeline\n",
    "nlp.pipe_names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7150e9e8",
   "metadata": {},
   "source": [
    "<img src=\"images/spacy-doc.png\">\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### Components of the Pipeline:\n",
    "1. **Input:** `Text`\n",
    "2. **Processing Steps:**\n",
    "   - **tok2vec:** Converts tokens to vectors for further processing.\n",
    "   - **tagger:** Assigns Part of Speech (POS) tags to tokens.\n",
    "   - **parser:** Analyzes the syntactic structure of the sentence.\n",
    "   - **attribute_ruler:** Applies rules to set linguistic attributes.\n",
    "   - **lemmatizer:** Converts tokens to their base forms (lemmas).\n",
    "   - **ner (Named Entity Recognizer):** Identifies and labels named entities.\n",
    "3. **Output:** `Doc` object, containing all processed information.\n",
    "\n",
    "### Key Points:\n",
    "- A **pipe** is an individual component of the pipeline, responsible for specific tasks.\n",
    "- **Pipeline Workflow:**\n",
    "  - The **tokenizer** splits the text into individual tokens.\n",
    "  - The **parser** structures the text syntactically.\n",
    "  - The **NER** module identifies entities and assigns appropriate labels.\n",
    "- The **Doc** object stores all the processed data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3fb79f80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['CARDINAL',\n",
       " 'DATE',\n",
       " 'EVENT',\n",
       " 'FAC',\n",
       " 'GPE',\n",
       " 'LANGUAGE',\n",
       " 'LAW',\n",
       " 'LOC',\n",
       " 'MONEY',\n",
       " 'NORP',\n",
       " 'ORDINAL',\n",
       " 'ORG',\n",
       " 'PERCENT',\n",
       " 'PERSON',\n",
       " 'PRODUCT',\n",
       " 'QUANTITY',\n",
       " 'TIME',\n",
       " 'WORK_OF_ART']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The pre-trained model supports the following entities\n",
    "nlp.pipe_labels['ner']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f519a31f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95d82257",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1d47f129",
   "metadata": {},
   "source": [
    "<h5 align=\"center\"><div class=\"alert alert-success\" style=\"margin: 20px\">Ehtisham works full time at Rivon.ai since March 2005 and part time in twitter</h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d329b03a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ehtisham ---> PERSON\n",
      "Rivon.ai ---> ORG\n",
      "March 2005 ---> DATE\n"
     ]
    }
   ],
   "source": [
    "mystr = \"Ehtisham works full time at Rivon.ai since March 2005 and part time in twitter\"\n",
    "# Process the text using the spaCy pipeline\n",
    "doc = nlp(mystr)\n",
    "# Extract and display named entities\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, \"--->\", ent.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ehtisham ---> PERSON\n",
      "Rivon.ai ---> ORG\n",
      "March 2005 ---> DATE\n",
      "Twitter Inc. ---> ORG\n"
     ]
    }
   ],
   "source": [
    "mystr = \"Ehtisham works full time at Rivon.ai since March 2005 and part time in Twitter Inc.\"\n",
    "# Process the text using the spaCy pipeline\n",
    "doc = nlp(mystr)\n",
    "# Extract and display named entities\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, \"--->\", ent.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "043d261a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "aa86f010",
   "metadata": {},
   "source": [
    "**Named Entity Recognition Annotations in spaCy**\n",
    "\n",
    "To perform NER in spaCy, instantiate a spaCy object as `doc`. The `doc.ents` are token spans with their own set of annotations. These annotations provide detailed information about the entities identified.\n",
    "\n",
    "### Annotations for Named Entities:\n",
    "| Attribute           | Description                                              |\n",
    "|---------------------|----------------------------------------------------------|\n",
    "| **`ent.text`**      | The original entity text                                  |\n",
    "| **`ent.label`**     | The entity type's hash value                              |\n",
    "| **`ent.label_`**    | The entity type's string description                      |\n",
    "| **`ent.start`**     | The token span's *start* index position in the `Doc`      |\n",
    "| **`ent.end`**       | The token span's *stop* index position in the `Doc`       |\n",
    "| **`ent.start_char`**| The entity text's *start* index position in the `Doc`     |\n",
    "| **`ent.end_char`**  | The entity text's *stop* index position in the `Doc`      |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eacf2d40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entity: Ehtisham\n",
      "  - Label (Hash): 380\n",
      "  - Label (String): PERSON\n",
      "  - Start Index (Token): 0\n",
      "  - End Index (Token): 1\n",
      "  - Start Index (Character): 0\n",
      "  - End Index (Character): 8\n",
      "\n",
      "Entity: Rivon.ai\n",
      "  - Label (Hash): 383\n",
      "  - Label (String): ORG\n",
      "  - Start Index (Token): 5\n",
      "  - End Index (Token): 6\n",
      "  - Start Index (Character): 28\n",
      "  - End Index (Character): 36\n",
      "\n",
      "Entity: March 2005\n",
      "  - Label (Hash): 391\n",
      "  - Label (String): DATE\n",
      "  - Start Index (Token): 7\n",
      "  - End Index (Token): 9\n",
      "  - Start Index (Character): 43\n",
      "  - End Index (Character): 53\n",
      "\n",
      "Entity: Twitter Inc.\n",
      "  - Label (Hash): 383\n",
      "  - Label (String): ORG\n",
      "  - Start Index (Token): 13\n",
      "  - End Index (Token): 15\n",
      "  - Start Index (Character): 71\n",
      "  - End Index (Character): 83\n",
      "\n"
     ]
    }
   ],
   "source": [
    "text = \"Ehtisham works full time at Rivon.ai since March 2005 and part time in Twitter Inc.\"\n",
    "\n",
    "# Process the text\n",
    "doc = nlp(text)\n",
    "\n",
    "# Extract and print entity annotations\n",
    "for ent in doc.ents:\n",
    "    print(f\"Entity: {ent.text}\")\n",
    "    print(f\"  - Label (Hash): {ent.label}\")\n",
    "    print(f\"  - Label (String): {ent.label_}\")\n",
    "    print(f\"  - Start Index (Token): {ent.start}\")\n",
    "    print(f\"  - End Index (Token): {ent.end}\")\n",
    "    print(f\"  - Start Index (Character): {ent.start_char}\")\n",
    "    print(f\"  - End Index (Character): {ent.end_char}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1f2e988",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a4dde37",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "58c0489c",
   "metadata": {},
   "source": [
    "**Example 2: Adding a Named Entity to a Span**\n",
    "\n",
    "<h5 align=\"center\"><div class=\"alert alert-success\" style=\"margin: 20px\">Ehtisham has moved to Karachi where he will be playing basketball on 25 December 2024.</h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d1237e57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\n",
      "Entity Text                    Entity Label    Description\n",
      "\u001b[m\n",
      "Ehtisham                       ORG             Companies, agencies, institutions, etc.\n",
      "Karachi                        ORG             Companies, agencies, institutions, etc.\n",
      "25 December 2024               DATE            Absolute or relative dates or periods\n"
     ]
    }
   ],
   "source": [
    "# Input sentence\n",
    "mystr = \"Ehtisham has moved to Karachi where he will be playing basketball on 25 December 2024.\"\n",
    "\n",
    "# Process the sentence using spaCy\n",
    "doc = nlp(mystr)\n",
    "\n",
    "# Print the entities identified by spaCy\n",
    "print('\\033[1m')\n",
    "print(f'{\"Entity Text\":{30}} {\"Entity Label\":{15}} {\"Description\"}')\n",
    "print('\\033[m')\n",
    "\n",
    "for ent in doc.ents:\n",
    "    print(f'{ent.text:{30}} {ent.label_:{15}} {spacy.explain(ent.label_)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "027de75e",
   "metadata": {},
   "source": [
    "**Observations**\n",
    "\n",
    "- Misclassifies playing as an entity EVENT.\n",
    "- Labels basketball as PRODUCT.\n",
    "- Misclassifies Karachi as ORG instead of GPE.\n",
    "\n",
    "**Next Steps**\n",
    "- We can manually correct and add the missing entities in the spaCy Doc object to improve the NER process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecc4672b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7f032106",
   "metadata": {},
   "source": [
    "**Adding and Correcting Named Entities using spaCy's `Span` Class**\n",
    "- The `Span` class in spaCy is used to create a span of tokens within a document (`doc` object).\n",
    "- A span is assigned a label to identify it as a specific type of named entity.\n",
    "- Spans are useful for adding or correcting entities in a `doc` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "35ba6ebb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "basketball\n",
      "<class 'spacy.tokens.span.Span'>\n"
     ]
    }
   ],
   "source": [
    "# Check a specific token span in the doc\n",
    "from spacy.tokens import Span\n",
    "print(doc[10:11])  # Output: basketball\n",
    "print(type(doc[10:11]))  # Output: spacy.tokens.span.Span"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fa60e873",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a span for new or corrected entities\n",
    "# 0 is the start index position of the span, 1 is the stop index position (exclusive)\n",
    "s1 = Span(doc, 9, 10, label='EVENT')  # Span for 'playing'\n",
    "s2 = Span(doc, 10, 11, label='PRODUCT')  # Span for 'basketball'\n",
    "s3 = Span(doc, 4, 5, label='GPE')  # Span for 'Karachi'\n",
    "\n",
    "# Updating the doc with these spans\n",
    "doc.set_ents([s1, s2, s3], default='unmodified')  # 'unmodified' keeps current state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fec85dc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\n",
      "Entity Text                    Entity Label    Description\n",
      "\u001b[m\n",
      "Ehtisham                       ORG             Companies, agencies, institutions, etc.\n",
      "Karachi                        GPE             Countries, cities, states\n",
      "playing                        EVENT           Named hurricanes, battles, wars, sports events, etc.\n",
      "basketball                     PRODUCT         Objects, vehicles, foods, etc. (not services)\n",
      "25 December 2024               DATE            Absolute or relative dates or periods\n"
     ]
    }
   ],
   "source": [
    "# Printing the updated entities\n",
    "print('\\033[1m')\n",
    "print(f'{\"Entity Text\":{30}} {\"Entity Label\":{15}} {\"Description\"}')\n",
    "print('\\033[m')\n",
    "\n",
    "for ent in doc.ents:\n",
    "    print(f'{ent.text:{30}} {ent.label_:{15}} {spacy.explain(ent.label_)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dc74f07",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1207589",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "13dfc842",
   "metadata": {},
   "source": [
    "**Example 3: Using spaCy for Named Entity Recognition (NER) on Medical Text**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "613a6899",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ali ---> PERSON\n",
      "Pneumonia ---> GPE\n"
     ]
    }
   ],
   "source": [
    "# Input medical text\n",
    "mystr = \"Dr. Ali has diagnosed that the patient is suffering from Pneumonia. \\\n",
    "Patient's left lung is inflamed and the air sacs in the right lung seem to be filled with mucus. \\\n",
    "Patient is advised to have plenty of water and antibiotics. In case of chest pain, take painkillers.\"\n",
    "\n",
    "# Apply spaCy model\n",
    "doc = nlp(mystr)\n",
    "\n",
    "# Print recognized entities and their labels\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, \"--->\", ent.label_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09cc2b3c",
   "metadata": {},
   "source": [
    "**Observations**\n",
    "- The spaCy built-in ner pipeline did not accurately perform NER on this clinical or medical text.\n",
    "- This is because the pre-trained en_core_web_sm model was not specifically trained on medical or clinical datasets.\n",
    "\n",
    "**Techniques to Build Custom NER for Medical Text**\n",
    "\n",
    "1. **Simple Lookup Using Dictionary-Based NER:** Use a dictionary or gazetteer containing medical terms, diseases, and conditions for entity recognition.\n",
    "2. **Rule-Based NER:** Write specific rules and patterns (e.g., using regular expressions) to identify entities in medical text. `Example:` Use rules to detect diseases, symptoms, and drug names.\n",
    "3. **Machine Learning-Based NER:** Train a custom NER model using labeled medical datasets. Use frameworks like spaCy or other libraries to fine-tune models on domain-specific data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e1e273a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2a5c7fe0",
   "metadata": {},
   "source": [
    "## c. Visualizing Named Entities using `displacy`\n",
    "\n",
    "To visually render Named Entities, we can use the `displacy` module of spaCy. The module provides easy visualization of recognized entities.\n",
    "\n",
    "##### Methods:\n",
    "- **On Jupyter Notebook**: Use `displacy.render()`\n",
    "- **On Other IDEs**: Use `displacy.serve()`\n",
    "- For more details, visit: [spaCy Visualizers Documentation](https://spacy.io/usage/visualizers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e1e18ad",
   "metadata": {},
   "source": [
    "**Example 1**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b7ec30a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">The \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    NIKE\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       " company’s revenue for \n",
       "<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    FY 2020\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">DATE</span>\n",
       "</mark>\n",
       " is \n",
       "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    $27 million\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">MONEY</span>\n",
       "</mark>\n",
       ". There is a need for \n",
       "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    $4 billion\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">MONEY</span>\n",
       "</mark>\n",
       " in new taxes. Budget \n",
       "<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    2019\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">DATE</span>\n",
       "</mark>\n",
       ": $\n",
       "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    39\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">MONEY</span>\n",
       "</mark>\n",
       ". \n",
       "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    One billion dollars\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">MONEY</span>\n",
       "</mark>\n",
       ". During \n",
       "<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    the current fiscal year\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">DATE</span>\n",
       "</mark>\n",
       ", \n",
       "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    36.70 million dollars\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">MONEY</span>\n",
       "</mark>\n",
       " were spent. The market for the \n",
       "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    U.S.\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
       "</mark>\n",
       " is projected at \n",
       "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    $4 billion\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">MONEY</span>\n",
       "</mark>\n",
       " in \n",
       "<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    2019\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">DATE</span>\n",
       "</mark>\n",
       ". It is estimated that the global economy will be worth \n",
       "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    $4 billion\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">MONEY</span>\n",
       "</mark>\n",
       " by \n",
       "<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    2020\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">DATE</span>\n",
       "</mark>\n",
       ".</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from spacy import displacy\n",
    "\n",
    "# Input text\n",
    "mystr = \"The NIKE company’s revenue for FY 2020 is $27 million. \\\n",
    "There is a need for $4 billion in new taxes. Budget 2019: $39. \\\n",
    "One billion dollars. During the current fiscal year, 36.70 million dollars were spent. \\\n",
    "The market for the U.S. is projected at $4 billion in 2019. \\\n",
    "It is estimated that the global economy will be worth $4 billion by 2020.\"\n",
    "\n",
    "# Process the text\n",
    "doc = nlp(mystr)\n",
    "\n",
    "# Visualize the entities\n",
    "displacy.render(doc, style='ent', jupyter=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6754f74",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3db3ca42",
   "metadata": {},
   "source": [
    "**Example 2: Viewing Only Specific Documents**\n",
    "- We can pass list of entity types to restrict visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dcc3bee6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">The \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    NIKE\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       " company’s revenue for \n",
       "<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    FY 2020\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">DATE</span>\n",
       "</mark>\n",
       " is $27 million. There is a need for $4 billion in new taxes. Budget \n",
       "<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    2019\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">DATE</span>\n",
       "</mark>\n",
       ": $39. One billion dollars. During \n",
       "<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    the current fiscal year\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">DATE</span>\n",
       "</mark>\n",
       ", 36.70 million dollars were spent. The market for the U.S. is projected at $4 billion in \n",
       "<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    2019\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">DATE</span>\n",
       "</mark>\n",
       ". It is estimated that the global economy will be worth $4 billion by \n",
       "<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    2020\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">DATE</span>\n",
       "</mark>\n",
       ".</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from spacy import displacy\n",
    "\n",
    "# Input text\n",
    "mystr = \"The NIKE company’s revenue for FY 2020 is $27 million. \\\n",
    "There is a need for $4 billion in new taxes. Budget 2019: $39. \\\n",
    "One billion dollars. During the current fiscal year, 36.70 million dollars were spent. \\\n",
    "The market for the U.S. is projected at $4 billion in 2019. \\\n",
    "It is estimated that the global economy will be worth $4 billion by 2020.\"\n",
    "\n",
    "# Process the text\n",
    "doc = nlp(mystr)\n",
    "\n",
    "options = {'ents':['ORG', 'DATE']}\n",
    "# Visualize the entities\n",
    "displacy.render(doc, style='ent', jupyter=True, options=options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f78d36cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b170ed19",
   "metadata": {},
   "source": [
    "# 5. Coreference Resolution\n",
    "\n",
    "## a. What is Coreference Resolution?\n",
    "- **Coreference Resolution** is a process of identifying all noun phrases that refer to the same entity in a text.\n",
    "\n",
    "\n",
    "<h5 align=\"center\"><div class=\"alert alert-success\" style=\"margin: 20px\">I voted for Imran Khan, because he was most aligned with my values.</h5>\n",
    "\n",
    "<img src=\"images/coref-resolution1.png\" height=300px width=1000px>\n",
    "\n",
    "- In this example:\n",
    "  - The pronoun **\"he\"** refers back to **\"Imran Khan\"**.\n",
    "  - The pronoun **\"my\"** refers to the speaker, **\"I\"**.\n",
    "  - Coreference resolution establishes these links between words and their referenced entities."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "921bea2d",
   "metadata": {},
   "source": [
    "#### Explanation of the Example:\n",
    "1. **Input Text**: \"I voted for Imran Khan, because he was most aligned with my values.\"\n",
    "2. **Coreference Links**:\n",
    "   - **\"I\"** (PRONOMINAL) → The speaker.\n",
    "   - **\"Imran Khan\"** (PROPER) → The candidate being referred to.\n",
    "   - **\"he\"** (PRONOMINAL) → Refers back to \"Imran Khan\".\n",
    "   - **\"my\"** (PRONOMINAL) → Refers back to \"I\".\n",
    "   - **\"values\"** (NOMINAL) → Refers to the speaker's beliefs or ideals.\n",
    "3. **Visualization Details**:\n",
    "   - Red and gray arcs show the relationships and confidence levels for these coreference connections.\n",
    "   - The numbers above the arcs represent the confidence scores or probabilities for the connections."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08e8d2a5",
   "metadata": {},
   "source": [
    "#### Coreference Resolution Tools:\n",
    "- Tools like `NeuralCoref` and `spaCy` can be used for performing coreference resolution.\n",
    "- To explore coreference resolution further, visit: [Hugging Face - Coreference Resolution](https://huggingface.co/coref)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f7706b5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "977b2131",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7dc72d73",
   "metadata": {},
   "source": [
    "## b. Why Coreference Resolution?\n",
    "\n",
    "Coreference resolution is essential for various Natural Language Processing (NLP) tasks because it helps in:\n",
    "\n",
    "- **Understanding Text Meaning**: Resolves pronouns and noun references to clarify relationships between entities in a sentence.\n",
    "- **Question Answering**: Helps systems resolve ambiguity in questions by linking entities to their context.\n",
    "- **Text Summarization**: Ensures coherent summaries by linking pronouns or phrases to their respective entities.\n",
    "- **Information Extraction**: Enhances the quality of extracted data by identifying related entities.\n",
    "- **Machine Translation**: Improves the accuracy of translations by resolving references across sentences.\n",
    "- **Chatbots and Dialogue Systems**: Enables better understanding of user context in conversations.\n",
    "\n",
    "---\n",
    "\n",
    "## c. How to Do Coreference Resolution?\n",
    "<img src=\"images/spacy-doc.png\" height=300px width=900px>\n",
    "\n",
    "There are various approaches to perform coreference resolution in NLP:\n",
    "\n",
    "##### 1. **Rule-Based Approaches**\n",
    "   - Create rules to identify references. For example:\n",
    "     - A pronoun like \"he\" often refers to the nearest male noun in context.\n",
    "   - Effective for simple, specific use cases but lacks generalizability.\n",
    "\n",
    "##### 2. **Machine Learning-Based Approaches**\n",
    "   - Train models to identify references using features like:\n",
    "     - **Grammatical roles**.\n",
    "     - **Distance between pronoun and reference**.\n",
    "     - **Semantic similarity**.\n",
    "   - Models such as Conditional Random Fields (CRFs) and Support Vector Machines (SVMs) have been used historically.\n",
    "\n",
    "##### 3. **Neural Network-Based Approaches**\n",
    "   - Use advanced models like transformers for better understanding of the context.\n",
    "   - Examples include:\n",
    "     - **NeuralCoref**: A coreference resolution library built on top of spaCy.\n",
    "     - Hugging Face models for more advanced use cases."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e421a3c5",
   "metadata": {},
   "source": [
    "**Example Using NeuralCoref:**\n",
    "To perform coreference resolution using NeuralCoref, follow these steps:\n",
    "1. Install NeuralCoref:\n",
    "   ```\n",
    "   pip install neuralcoref\n",
    "   ````"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cd69c25",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e499c4c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cad88a7b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b565c01",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "581a0efc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "011d914c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7eaef413-a6ff-4f44-9255-46b36483428e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <style>\n",
       "        body {\n",
       "            background-color: #f2fff2;\n",
       "        }\n",
       "        h1 {\n",
       "            text-align: center;\n",
       "            font-weight: bold;\n",
       "            font-size: 36px;\n",
       "            color: #4295F4;\n",
       "            text-decoration: underline;\n",
       "            padding-top: 15px;\n",
       "        }\n",
       "        \n",
       "        h2 {\n",
       "            text-align: left;\n",
       "            font-weight: bold;\n",
       "            font-size: 30px;\n",
       "            color: #4A000A;\n",
       "            text-decoration: underline;\n",
       "            padding-top: 10px;\n",
       "        }\n",
       "        \n",
       "        h3 {\n",
       "            text-align: left;\n",
       "            font-weight: bold;\n",
       "            font-size: 30px;\n",
       "            color: #f0081e;\n",
       "            text-decoration: underline;\n",
       "            padding-top: 5px;\n",
       "        }\n",
       "\n",
       "        \n",
       "        p {\n",
       "            text-align: center;\n",
       "            font-size: 12 px;\n",
       "            color: #0B9923;\n",
       "        }\n",
       "    </style>\n",
       "\n",
       "<h1>Hello</h1>\n",
       "<p>Hello World</p>\n",
       "<h2> Hello</h2>\n",
       "<h3> World </h3>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.core.display import HTML\n",
    "\n",
    "style = \"\"\"\n",
    "    <style>\n",
    "        body {\n",
    "            background-color: #f2fff2;\n",
    "        }\n",
    "        h1 {\n",
    "            text-align: center;\n",
    "            font-weight: bold;\n",
    "            font-size: 36px;\n",
    "            color: #4295F4;\n",
    "            text-decoration: underline;\n",
    "            padding-top: 15px;\n",
    "        }\n",
    "        \n",
    "        h2 {\n",
    "            text-align: left;\n",
    "            font-weight: bold;\n",
    "            font-size: 30px;\n",
    "            color: #4A000A;\n",
    "            text-decoration: underline;\n",
    "            padding-top: 10px;\n",
    "        }\n",
    "        \n",
    "        h3 {\n",
    "            text-align: left;\n",
    "            font-weight: bold;\n",
    "            font-size: 30px;\n",
    "            color: #f0081e;\n",
    "            text-decoration: underline;\n",
    "            padding-top: 5px;\n",
    "        }\n",
    "\n",
    "        \n",
    "        p {\n",
    "            text-align: center;\n",
    "            font-size: 12 px;\n",
    "            color: #0B9923;\n",
    "        }\n",
    "    </style>\n",
    "\"\"\"\n",
    "\n",
    "html_content = \"\"\"\n",
    "<h1>Hello</h1>\n",
    "<p>Hello World</p>\n",
    "<h2> Hello</h2>\n",
    "<h3> World </h3>\n",
    "\"\"\"\n",
    "\n",
    "HTML(style + html_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a2a1f78-cc5c-45b8-8555-ab23ac753b2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a87ee7ba",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
