{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d162b058",
   "metadata": {},
   "source": [
    "<h1 align=\"center\">Basic Text Pre-Processing</h1>\n",
    "<h3><div align=\"right\">Ehtisham Sadiq</div></h3>    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19b0c47d",
   "metadata": {},
   "source": [
    "<img align=\"center\" width=\"900\"  src=\"images/phase2.png\"  > "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e23a6901",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2c17026",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9efdaacd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1b8ced6b",
   "metadata": {},
   "source": [
    "# Learning agenda of this notebook\n",
    "1. **Text Cleaning**\n",
    "    - Removing digits and words containing digits\n",
    "    - Removing newline characters and extra spaces\n",
    "    - Removing HTML tags\n",
    "    - Removing URLs\n",
    "    - Removing punctuations\n",
    "    \n",
    "\n",
    "2. **Basic Text Preprocessing**\n",
    "    - Case folding\n",
    "    - Expand contractions\n",
    "    - Chat word treatment\n",
    "    - Handle emojis\n",
    "    - Spelling correction\n",
    "    - Tokenization\n",
    "    - Creating N-grams\n",
    "    - Stop words Removal\n",
    " \n",
    " \n",
    "3. **Advanced Preprocessing**\n",
    "    - Stemming\n",
    "    - Lemmatization\n",
    "    - POS tagging\n",
    "    - NER\n",
    "    - Parsing\n",
    "    - Coreference Resolution\n",
    "    \n",
    "\n",
    "4. **Text Pre-Processing on Tweets Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3b5f1e7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c070ed94",
   "metadata": {},
   "source": [
    "# Download and Install Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89c00384",
   "metadata": {},
   "outputs": [],
   "source": [
    "!{sys.executable} -m pip install -q --upgrade pip\n",
    "!{sys.executable} -m pip install -q --upgrade numpy pandas sklearn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94d075b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sys\n",
    "!pip install gensim wordcloud textblob contractions clean-text unicode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc38c31a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79ba6784",
   "metadata": {},
   "source": [
    "# 1. Text Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29fcb284",
   "metadata": {},
   "source": [
    "## a. Removing Digits and Words Containing Digits \n",
    "- Sometimes it happens that words and digits combine are written in the text which creates a problem for machines to understand. Hence, we need to remove the words and digits which are combined like game57 or game5ts7.\n",
    "- For such and many other tasks we normally use Regular Expressions.\n",
    "\n",
    "- The **`re.sub(pattern, replacement_string, str)`** method return the string obtained by replacing the occurrences of `pattern` in `str` with the `replacement_string`. If the pattern isn‚Äôt found, the string is returned unchanged."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3827cbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "mystr = \"This is abc32 a abc32xyz string containing 32abc words  32 having digits\"\n",
    "re.sub('\\w*\\d\\w*', '', mystr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b93101cc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4d279225",
   "metadata": {},
   "source": [
    "## b. Removing New Line Characters and Extra Spaces\n",
    "- Most of the time text data contain extra spaces or while removing digits more than one space is left between the text.\n",
    "- We can use Python's string and re module to perform this pre-processing task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9f5e972",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "mystr = \"      This         is a       string  with   lots of   extra spaces      in beteween    words     .\"\n",
    "re.sub(' +', ' ', mystr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "785a2d73",
   "metadata": {},
   "outputs": [],
   "source": [
    "mystr = \"This is\\na string\\nwith lots of new\\nline characters.\"\n",
    "print(\"Original String:\\n\", mystr)\n",
    "print(\"Preprocessed String:\", re.sub('\\n', ' ', mystr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a52015e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3dee5c0a",
   "metadata": {},
   "source": [
    "## c. Removing HTML Tags\n",
    "- Once you get data via scraping websites, your data might contain HTML tags, which are not required as such in the data. So we need to remove them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "682dcc37",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "mystr = \"<html> <head> An empty head. </head><body><p> This is so simple and fun. </p> </body> </html>\"\n",
    "print(\"Original String: \", mystr)\n",
    "print(\"Preprocessed String: \", re.sub('<.*?>', '', mystr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ca00acb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "167b6180",
   "metadata": {},
   "source": [
    "## d. Removing URLs\n",
    "- At times the text data you have some URLS, which might not be helpful in suppose sentiment analysis. So better to remove those URLS from your dataset\n",
    "- Once again, we can use Python's re module to remove the URLs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74f73fc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "mystr = \"Good lectures by Ehtisham are available at http://www.youtube.com/c/Ehtisham/playlists\"\n",
    "re.sub('https?://\\S+|www.\\.\\S+', '', mystr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb9c503b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a3e0e219",
   "metadata": {},
   "source": [
    "## e. Removing Punctuations\n",
    "- Punctuations are symbols that are used to divide written words into sentences and clauses\n",
    "- Once you tokenize your text, these punctuation symbols may become part of a token, and may become a token by itself, which is not required in most of the cases\n",
    "- We can use Python's `string.punctuation` constant and `replace()` method to replace any punctuation in text with an empty string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b06a5f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "string.punctuation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0067f95f",
   "metadata": {},
   "source": [
    ">- Check for other constants like `string.whitespace`, `string.printable`, `string.ascii_letters`, `string.digits` as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f123fd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "mystr = 'A {text} ^having$ \"lot\" of #s and [puncutations]!.;%..'\n",
    "mystr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76aa3beb",
   "metadata": {},
   "outputs": [],
   "source": [
    "newstr = ''.join([ch for ch in mystr if ch not in string.punctuation])\n",
    "newstr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df149851",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "76f28d73",
   "metadata": {},
   "source": [
    "# 2. Basic Text Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86b42da8",
   "metadata": {},
   "source": [
    "## a. Case Folding \n",
    "- The text we need to process may come in lower, upper, sentence, camel cases\n",
    "- If the text is in the same case, it is easy for a machine to interpret the words because the lower case and upper case are treated differently by the machine\n",
    "- In applications like Information Retrieval, we reduce all letters to lower case\n",
    "- In applications like sentiment analysis, machine translation and information extraction, keeping the case might be helpful. For example US vs us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbb6c75e",
   "metadata": {},
   "outputs": [],
   "source": [
    "mystr = \"This IS GREAT series of Lectures by Ehtisham at the Deaprtment of Python\"\n",
    "mystr.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12b34d7d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "97fd645b",
   "metadata": {},
   "source": [
    "## b. Expand Contractions\n",
    "- Contractions are words or combinations of words that are shortened by dropping letters and replacing them by an apostrophe.\n",
    "- Examples:\n",
    "    - you're ---> you are\n",
    "    - ain't ---> am not / are not / is not / has not / have not\n",
    "    - you'll ---> you shall / you will\n",
    "    - wouldn't 've ---> would not haveyou are\n",
    "- In order to expand contractions, you can install and use the `contractions` module or can create your own dictionary to expand contractions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c90f198",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install -q  contractions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b5ff1ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import contractions\n",
    "print(contractions.fix(\"you're\"))      # you are\n",
    "print(contractions.fix(\"ain't\"))       # am not / are not / is not / has not / have not\n",
    "print(contractions.fix(\"you'll\"))      #you shall / you will\n",
    "print(contractions.fix(\"wouldn't've\")) #\"wouldn't've\": \"would not have\","
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fb8cc71",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6b0d807",
   "metadata": {},
   "outputs": [],
   "source": [
    "mystr = '''I'll be there within 5 min. Shouldn't you be there too? I'd love to see u there my dear. \n",
    "It's awesome to meet new friends. We've been waiting for this day for so long.'''\n",
    "mystr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "128dec87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use loop\n",
    "mylist = []   \n",
    "for word in mystr.split(sep=' '):\n",
    "    mylist.append(contractions.fix(word))\n",
    "\n",
    "newstring = ' '.join(mylist)\n",
    "print(newstring)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61d4bf5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use list comprehension and join the words of list on space\n",
    "expanded_string = ' '.join([contractions.fix(word) for word in mystr.split()])\n",
    "expanded_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d70a0fc7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c52da3de",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b864e428",
   "metadata": {},
   "source": [
    "## c. Chat Word Treatment\n",
    "- Some commonly used abbreviated chat words that are used on social media these days are:\n",
    "    - GN for good night\n",
    "    - fyi for for your information\n",
    "    - asap for as soon as possible\n",
    "    - yolo for you only live once\n",
    "    - rofl for rolling on floor laughing\n",
    "    - nvm for never mind\n",
    "    - ofc for ofcourse\n",
    "\n",
    "- To pre-process any text containing such abbreviations we can search for an online dictionary, or can create a dictionary of our own"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d8f4c40",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_chatwords = { \n",
    "    'ack': 'acknowledge',\n",
    "    'omg': 'oh my God',\n",
    "    'aisi': 'as i see it',\n",
    "    'bi5': 'back in 5 minutes',\n",
    "    'lmk': 'let me know',\n",
    "    'gn' : 'good night',\n",
    "    'fyi': 'for your information',\n",
    "    'asap': 'as soon as possible',\n",
    "    'yolo': 'you only live once',\n",
    "    'rofl': 'rolling on floor laughing',\n",
    "    'nvm': 'never ming',\n",
    "    'ofc': 'ofcourse',\n",
    "    'blv' : 'boulevard',\n",
    "    'cir' : 'circle',\n",
    "    'hwy' : 'highway',\n",
    "    'ln' : 'lane',\n",
    "    'pt' : 'point',\n",
    "    'rd' : 'road',\n",
    "    'sq' : 'square',\n",
    "    'st' : 'street'\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "560fb80d",
   "metadata": {},
   "outputs": [],
   "source": [
    "mystr = \"omg this is aisi I ack your work and will be bi5\"\n",
    "mystr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74246a90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dict.items() method returns all the key-value pairs of a dict as a two object tuple\n",
    "# dict.keys() method returns all the keys  of a dict object\n",
    "# dict.values() method returns all the values  of a dict object\n",
    "mylist = []   \n",
    "for word in mystr.split(sep=' '):\n",
    "    if word in dict_chatwords.keys():\n",
    "        mylist.append(dict_chatwords[word])\n",
    "    else:\n",
    "        mylist.append(word)\n",
    "newstring = ' '.join(mylist)\n",
    "print(newstring)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fee8b5a4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40839642",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "30f62553",
   "metadata": {},
   "source": [
    "## d. Handle Emojis\n",
    "- We come across lots and lots of emojis while scraping comments/posts from social media websites like Facebook, Instagram, Whatsapp, Twitter, LinkedIn, which needs to be removed from text.\n",
    "- Machine Learrning algorithm cannot understand emojis, so we have two options:\n",
    "    - Simply remove the emojis from the text data, and this can be done using `clean-text` library\n",
    "    - Replace the emoji with its meaning happy, sad, angry,....\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "620bf235",
   "metadata": {},
   "source": [
    "### (i) Remove Emojis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4294457",
   "metadata": {},
   "outputs": [],
   "source": [
    "mystr = \"These emojis needs to be removed, there is a huge list...üòÉüò¨üòÇüòÖüòáüòâüòäüòúüòéü§óüôÑü§îüò°üò§üò≠ü§†ü§°ü§´üí©üòàüëªüôåüëç‚úåÔ∏èüëåüôè\"\n",
    "mystr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8be07245",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    " \n",
    "emoji_pattern = re.compile(\"[\"\n",
    "        u\"\\U0001F600-\\U0001F64F\"  # code range for emoticons\n",
    "        u\"\\U0001F300-\\U0001F5FF\"  # code range for symbols & pictographs\n",
    "        u\"\\U0001F680-\\U0001F6FF\"  # code range for transport & map symbols\n",
    "        u\"\\U0001F1E0-\\U0001F1FF\"  # code range for flags (iOS)\n",
    "        u\"\\U00002700-\\U000027BF\"  # code range for Dingbats\n",
    "        u\"\\U00002500-\\U00002BEF\"  # code range for chinese char\n",
    "        u\"\\U00002702-\\U000027B0\"\n",
    "        u\"\\U00002702-\\U000027B0\"\n",
    "        u\"\\U000024C2-\\U0001F251\"\n",
    "        u\"\\U0001f926-\\U0001f937\"\n",
    "        u\"\\U00010000-\\U0010ffff\"\n",
    "        u\"\\u2640-\\u2642\" \n",
    "        u\"\\u2600-\\u2B55\"\n",
    "        u\"\\u200d\"\n",
    "        u\"\\u23cf\"\n",
    "        u\"\\u23e9\"\n",
    "        u\"\\u231a\"\n",
    "        u\"\\ufe0f\" \n",
    "        u\"\\u3030\"\n",
    "        \"]+\", flags=re.UNICODE)\n",
    "\n",
    "print(emoji_pattern.sub(r'', mystr)) # no emoji\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbbf5c6c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "42e7e881",
   "metadata": {},
   "source": [
    "### (ii) Replace Emojis with their Meanings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "059582dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install -q  emoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a02e204",
   "metadata": {},
   "outputs": [],
   "source": [
    "import emoji\n",
    "mystr = \"This is  üëç\"\n",
    "emoji.demojize(mystr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c6992f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "mystr = \"I am ü§î\"\n",
    "emoji.demojize(mystr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f4551af",
   "metadata": {},
   "outputs": [],
   "source": [
    "mystr = \"This is  üëç\"\n",
    "emoji.replace_emoji(mystr, replace='positive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdb98ff1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1037dc1e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "181c15ca",
   "metadata": {},
   "source": [
    "## e. Spelling Correction\n",
    "- Most of the times the text data you have contain spelling errors, which if not corrected the same word may be represented in two or may be more different ways.\n",
    "- Almost all word editors, today underline incorrectly typed words and provide you possible correct options\n",
    "- So spelling correction is a two step task:\n",
    "    - Detection of spelling errors\n",
    "    - Correction of spelling errors\n",
    "        - Autocorrect as you type space\n",
    "        - Suggest a single correct word\n",
    "        - Suggest a list of words (from which you can choose one)\n",
    "- Types of spelling errors:\n",
    "    - **Non-word Errors:** are non-dictionary words or words that do not exist in the language dictionary. For example instead of typing `reading` the user typed `reeding`. These are easy to detect as they do not exist in the language dictionary and can be corrected using algorithms like shortest weighted edit distance and highest noisy channel probability.\n",
    "    - **Real-word Errors:** are dictionary words and are hard to detect. These can be of two types:\n",
    "        - Typographical errors: For example instead of typing `great` the user typed `greet`\n",
    "        - Cognitive errors (homophones: For example instead of typing `two` the user typed `too`\n",
    "\n",
    "\n",
    "<h2 align=\"left\" style=\"font-family:'Arial'\">\"I am reeding thiss gret boook on deta sciance suject, which is a greet curse\"</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db931961",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a9985af",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install  numpy==1.21.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "458947c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install -q  textblob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dae6653-4c23-40ae-b6a1-e4005d66015b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install textblob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1362f338",
   "metadata": {},
   "outputs": [],
   "source": [
    "import textblob\n",
    "textblob.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89720d4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob\n",
    "mystr = \"I am reeding thiss gret boook on deta sciance suject, which is a greet curse\"\n",
    "blob = TextBlob(mystr)\n",
    "type(blob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af745e1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dir(blob))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4813310",
   "metadata": {},
   "outputs": [],
   "source": [
    "blob.correct().string"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01d58342",
   "metadata": {},
   "source": [
    ">-  The non-word errors like `reeding`, `thiss`, `gret`, `boook`, `deta`, `sciance` and `suject` have been corrected by `blob.correct()` method\n",
    ">- However, the real word errors like `greet` and `curse` are not corrected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6ed0157",
   "metadata": {},
   "outputs": [],
   "source": [
    "dir()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d0b85cf",
   "metadata": {},
   "source": [
    "**Let us try to understand how `textblob.correct()` method do this?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28586b2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The word attribute of textblob object returns list of words in the text\n",
    "blob.words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f7dd8a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word.spellcheck() method returns a list of (word, confidence) tuples with spelling suggestions\n",
    "# 'reeding'\n",
    "blob.words[2].spellcheck()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cea1786c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word.spellcheck() method returns a list of (word, confidence) tuples with spelling suggestions\n",
    "# 'boook'\n",
    "blob.words[5].spellcheck()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "782b98fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word.spellcheck() method returns a list of (word, confidence) tuples with spelling suggestions\n",
    "# 'greet'\n",
    "blob.words[13].spellcheck()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cf78a6c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b86328b4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "666d6448",
   "metadata": {},
   "source": [
    "## f. Tokenize Text\n",
    "\n",
    "<img align=right src=\"images/tokenization.png\" width=\"500\">\n",
    "\n",
    "- **What is Tokenization:** Tokenization is a process of splitting text into meaningful segments called tokens. It can be character level, subword level, word level (unigram), two word level (bigram), three word level (trigram), and sentence level.\n",
    "- **Why to do Tokenization:** For classification of a product review as positive or negative, we may need to count the number of positive words and compare them with the count of negative words in the text of that review. For this we first need to tokenize the text of the product review. Tokens are the basic uilding locks of a document oject. Everything that helps us understand the meaning of the text is derived from tokens and their relationship to one another.\n",
    "- **How to do Tokenization:** In a sentence you may come across following four items:\n",
    "    -  **Prefix**:\tCharacter(s) at the beginning &#9656; `( ‚Äú $ Rs Dr`\n",
    "    -  **Suffix**:\tCharacter(s) at the end &#9656; `km ) , . ! ‚Äù`\n",
    "    -  **Infix**:\tCharacter(s) in between &#9656; `- -- / ...`\n",
    "    -  **Exception**: Special-case rule to split a string into several tokens or prevent a token from being split when punctuation rules are applied. From `L.A.!` the exclamation mark (!) is separated, while `L.A.` is not split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14543dbd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e41f20c8",
   "metadata": {},
   "source": [
    "### (i) Tokenization with `string.split()` Method\n",
    "- The easiest way to tokenize is to use the `mystr.split()` method, which returns a list of strings.\n",
    "- The `mystr.split()` method splits a string into a list of strings at every occurrence of space character by default and discard empty strings from the result.\n",
    "- You may pass a parameter `sep='i'` to split method to split at that specific character instead.\n",
    "- It's limitation is that it do not  consider punctuation symbols as a separate token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6edf5df5",
   "metadata": {},
   "outputs": [],
   "source": [
    "mystr=\"Learning is fun with Ehtisham\" \n",
    "print(mystr.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "262a4014",
   "metadata": {},
   "outputs": [],
   "source": [
    "mystr=\"This example is great!\" \n",
    "print(mystr.split())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9e7855c",
   "metadata": {},
   "source": [
    "> <font color=green> Observe the output, the exclamation symbol has become part of the token great (which is wrong)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accd7219",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46c0bb69",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4f65b7fe",
   "metadata": {},
   "source": [
    "### (ii) Tokenization with `re.split()` Method\n",
    "- The `re.split()` method splits the source string by the occurrences of the pattern, returning a list containing the resulting substrings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78f6ce39",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "mystr=\"This example is great!\" \n",
    "pattern = re.compile(r'\\W+')\n",
    "pattern.split(mystr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a5b8f71",
   "metadata": {},
   "source": [
    ">- <font color=green> The exclamation symbol is not part of the token great, but what if I need that symbol as a separate token?\n",
    ">- <font color=green> Moreover, you need to write different regular expression for different scenarios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1cf7d66",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cf29a05",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2d2a51dd",
   "metadata": {},
   "source": [
    "### (iii) Tokenization using NLTK\n",
    "- NLTK stands for Natural Language Toolkit (https://www.nltk.org/). This is a suite of libraries and programs for statistical natural language processing for English language\n",
    "- NLTK was released in 2001, and is available for Windows, Mac OS X, and Linux.. \n",
    "- NLTK provides easy-to-use interfaces to over 50 corpora and lexical resources such as WordNet, along with a suite of text processing libraries for classification, tokenization, stemming, tagging, parsing, and semantic reasoning.\n",
    "- NLTK fully supports the English language, but others like Spanish or French are not supported as extensively.\n",
    "- It is a string processing libbrary, i.e., you give a string as input and get a string as output\n",
    "- There are. different tokenizer available in nltk:\n",
    "    - `nltk.tokenize.sent_tokenize(str)` for sentence tokenization\n",
    "    - `nltk.tokenize.word_tokenize(str)` for word tokenization\n",
    "    - `nltk.tokenize.treebank.TreebankWordTokenizer(str)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb11801b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install -q nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea7d7775",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd2a097d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "mystr=\"This example is great!\" \n",
    "print(word_tokenize(mystr))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8481ee7",
   "metadata": {},
   "source": [
    "> <font color=green> Observe the output, this time the exclamation symbol is kept as a separate tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16a08c24",
   "metadata": {},
   "outputs": [],
   "source": [
    "mystr=\"You should do your Ph.D in A.I!\" \n",
    "print(word_tokenize(mystr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c34b5bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "mystr=\"You should've sent me an email at ehtisham@rivon.edu.pk or vist http://www/ehtisham.me\"\n",
    "print(word_tokenize(mystr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d01562a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "mystr=\"Here's an example worth $100. I am 384400km away from earth's moon!\" \n",
    "print(word_tokenize(mystr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8673997",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7ddbac7d",
   "metadata": {},
   "source": [
    "### (iv) Tokenization with spaCy\n",
    "- **spaCy** (https://spacy.io/) is an open-source Natural Language Processing library designed to handle NLP tasks with the most efficient and state of the art algorithm, released in 2015. \n",
    "- Spacy support many languages (over 65) where you can perform tokenizing, however, for this other than importing spacy, you have to load the appropriate library using spacy.load() method. But before that make sure you have downloaded the model in your system.\n",
    "- spaCy will isolate punctuation that does *not* form an integral part of a word. Quotation marks, commas, and punctuation at the end of a sentence will be assigned their own token. However, punctuation that exists as part of an email address, website or numerical value will be kept as part of the token.\n",
    "\n",
    "- **Download spacy model for English language**\n",
    "    - Spacy comes with pretrained models and pipelines for different languages.\n",
    "    - You can download any of the following models for English language, but better to download the small as this will require a reasonable amount of space on your disk, and may take a bit of time to download:\n",
    "        - en_core_web_sm\n",
    "        - en_core_web_md\n",
    "        - en_core_web_lg\n",
    "        - en_core_web_trf\n",
    "    - The model name consist of four parts:\n",
    "        - Language (en): The language abreviation can be `en` for English, `fr` for French, `zh` for Chinese\n",
    "        - Type (core/dep): It can be core for general-purpose pipeline with tagging, parsing, lemmatization and NER recognition. It can be dep for only tagging, parsing and lemmatization\n",
    "        - Genre (web/news): It measn the type of text the pipeline is trained on, e.g., web or news. \n",
    "        - Size: Package size indicator. `sm` for small, `md` for medium, `lg` for large and `trf for transformer\n",
    "        - Package version (a.b.c): Here a is the major version for spaCy, b is the minor version for spaCy, while c is the model verion dependent to the data on which the model is trained, it parameters, number of iterations and different vectors.\n",
    "        \n",
    "> For details read spaCy101: https://spacy.io/usage/spacy-101"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed9f8834",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install -q spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5317e5b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "spacy.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed673218",
   "metadata": {},
   "source": [
    "**Download spacy model for English language**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cfeffff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ssl\n",
    "ssl._create_default_https_context = ssl._create_unverified_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e336a36",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "!{sys.executable} -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d24b3f4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f8a39956",
   "metadata": {},
   "source": [
    "**Example 1:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1eef602",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import spacy and load the language library\n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "mystr=\"'A 7km Uber cab ride from Gulberg to Joher Town will cost you $20\" \n",
    "doc = nlp(mystr)\n",
    "\n",
    "for token in doc:\n",
    "    print(token, end=' , ')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6b29abc",
   "metadata": {},
   "source": [
    "> <font color=green> Note that spacy has successfully tokenized the distance symbol, which nltk failed to separate.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "448839ee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8c3a09da",
   "metadata": {},
   "source": [
    "**Example 2:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0baf9f37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import spacy and load the language library\n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "mystr=\"You should've sent me an email at ehtisham@rivon.edu.pk or vist http://www/ehtisham.me\"\n",
    "doc = nlp(mystr)\n",
    "\n",
    "for token in doc:\n",
    "    print(token, end=' , ')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c46130c",
   "metadata": {},
   "source": [
    ">- <font color=green> Note that spacy has kept the email as a single token, while nltk separated it.</font>\n",
    ">- <font color=green> However, spacy also failed to properly tokenize the URL :(</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18cb5cc0",
   "metadata": {},
   "source": [
    "**Additional Token Attributes:** Once the string is passed to `nlp()` method of spacy, the tokens of the resulting `doc` object has many other associated attributes other than just tokens:\n",
    "\n",
    "|Tag|Description\n",
    "|:------|:------:\n",
    "|`.text`|The original word text\n",
    "|`.lemma_`|The base form of the word\n",
    "|`.pos_`|The simple part-of-speech tag\n",
    "|`.tag_`|The detailed part-of-speech tag\n",
    "|`.shape_`|The word shape ‚Äì capitalization, punctuation, digits\n",
    "|`.is_alpha`, `is_ascii`, `is_digit`|Token text consists of alphanumeric characters, ASCII characters, digits\n",
    "|`.is_lower`, `is_upper`, `is_title`|Token text is in lowercase, uppercase, titlecase\n",
    "|`.is_punct`, `is_space`, `is_stop`|Token is punctuation, whitespace, stopword"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e5bf4d8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0c0d692",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1d2ca27",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "13c60224",
   "metadata": {},
   "source": [
    "## g. Creating N-grams\n",
    "- **What are n-grams?** \n",
    "    - A sequence of n words, can be bigram, trigram,....\n",
    "- **Why to use n-grams?** \n",
    "    - Capture contextual information (`good food` carries more meaning than just `good` and `food` when observed independently)\n",
    "    - Applications of N-grams:\n",
    "        - Sentence Completion\n",
    "        - Auto Spell Check and correction\n",
    "        - Auto Grammer Check and correction\n",
    "    - Is there a perfect value of n?\n",
    "        - Different types of n-grams are suitable for different types of applications. You should try different n-grams on your data in order to confidently conclude which one works the best among all for your text analysis. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec36da1d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1b05d832",
   "metadata": {},
   "source": [
    "- **How to create n-grams?** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f32e3256",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "mystr = \"Allama Iqbal was a visionary philosopher and politician. Thank you\"\n",
    "tokens = nltk.tokenize.word_tokenize(mystr)\n",
    "bgs = nltk.bigrams(tokens)\n",
    "print(bgs)\n",
    "for grams in bgs:\n",
    "    print(grams)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f6b31af",
   "metadata": {},
   "source": [
    ">- The formula to calculate the count of n-grams in a document is: **`X - N + 1`**, where `X` is the number of words in a given document and `N` is the number of words in n-gram\n",
    "\\begin{equation}\n",
    "    \\text{Count of N-grams} \\hspace{0.5cm} = \\hspace{0.5cm} 11 - 2 + 1 \\hspace{0.5cm} = \\hspace{0.5cm} 10\n",
    "\\end{equation}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df993560",
   "metadata": {},
   "outputs": [],
   "source": [
    "tgs = nltk.trigrams(tokens)\n",
    "for grams in tgs:\n",
    "    print(grams)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43de30a5",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "    \\text{Count of N-grams} \\hspace{0.5cm} = \\hspace{0.5cm} 11 - 3 + 1 \\hspace{0.5cm} = \\hspace{0.5cm} 9\n",
    "\\end{equation}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30e32325",
   "metadata": {},
   "outputs": [],
   "source": [
    "ngrams = nltk.ngrams(tokens, 4)\n",
    "for grams in ngrams:\n",
    "    print(grams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cef11306",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1591bca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "94571d33",
   "metadata": {},
   "source": [
    "## h.  Stopwords Removal\n",
    "- Stopwords are extremely common words of a language having very little meanings, and it is usually safe to remove them and not consider them as important for later processing of our data.\n",
    "- Every language has its own set of stopwords. For example, some stopwords of English language are: the, a, an, was, were, at, will, on, in, from, to, me, you, yours,....\n",
    "- Whether you should remove stop words from your text or not mainly depends on the problem you are solving.\n",
    "- Remove stop words from your text if you are working on:\n",
    "    - Text Classification (Spam Filtering, Language Classification, Genre Classification)\n",
    "    - Caption Generation\n",
    "    - Auto-Tag Generation\n",
    "- Avoid removing stop words from your text if you are working on:\n",
    "    - Machine Translation\n",
    "    - Language Modeling\n",
    "    - Text Summarization\n",
    "    - Question-Answering problems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed7ab8fc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "75c1ad24",
   "metadata": {},
   "source": [
    "### (i) Using NLTK\n",
    "- The NLTK library has a defined set of stopwords for different languages like English. Here, we will focus on ‚Äòenglish‚Äô stopwords. One can also consider additional stopwords if required\n",
    "- Note that there is no single universal list of stopwords. The list of the stop words can change depending on your problem statement\n",
    "- Once you install nltk, it just install the base library and do not install all the packages related to different languages, different tokenization schemes, etc. To install all the nltk packages and corpora use `nltk.download()`\n",
    "- An installation window will pop up. Select all and click ‚ÄòDownload‚Äô to download and install the additional bundles. This will download all the dictionaries and other language and grammar data frames necessary for full NLTK functionality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53f5e0bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ssl\n",
    "ssl._create_default_https_context = ssl._create_unverified_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dbe631c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download(\"stopwords\")\n",
    "# nltk.download()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d80496a3",
   "metadata": {},
   "source": [
    "> After completion of downloading, you can load the package of `stopwords` from the `nltk.corpus` and use it to load the stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a41327c8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3294b743",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "print(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c23f76a8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32c2734e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39ef9810",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(text):\n",
    "    new_text = list()\n",
    "    for word in text.split():\n",
    "        if word not in stopwords.words('english'):\n",
    "            new_text.append(word)\n",
    "    return \" \".join(new_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88fa5ee1",
   "metadata": {},
   "source": [
    "**Removing Stopwords from Text of an Email**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "813eb042",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "mystr=\"Your Google account has been compromised. \\\n",
    "    Your account will be closed. Immediately click this link to update your account\"\n",
    "remove_stopwords(mystr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c9b91e9",
   "metadata": {},
   "source": [
    "**Removing Stopwords for a Sentiment Analysis Application**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2293ec5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "mystr=\"This movie is not good\"\n",
    "remove_stopwords(mystr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37d0b2be",
   "metadata": {},
   "source": [
    ">- <font color=green>For sentiment analysis purposes, the overall meaning of the resulting sentence is positive, which is not at all the reality. So either do not remove sentiment analysis while doing sentiment analysis or handle the negation before removing stopwords "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61521760",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e374466e",
   "metadata": {},
   "source": [
    "### (ii) Using spaCy\n",
    "- **spaCy** (https://spacy.io/) is an open-source Natural Language Processing library designed to handle NLP tasks with the most efficient and state of the art algorithm, released in 2015. \n",
    "- Spacy support many languages (over 65) where you can perform tokenizing, however, for this other than importing spacy, you have to load the appropriate library using spacy.load() method. But before that make sure you have downloaded the model in your system.\n",
    "- **Download spacy model for English language:** Spacy comes with pretrained models and pipelines for different languages. We have already downloaded the pre-trained spacy model for English language\n",
    "> For details read spaCy101: https://spacy.io/usage/spacy-101"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62844349",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4b7db56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# returns a set of around 326 English stopwords built into spaCy\n",
    "print(len(nlp.Defaults.stop_words))\n",
    "print(nlp.Defaults.stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "893c079e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords_spacy(text):\n",
    "    new_text = list()\n",
    "    for word in text.split():\n",
    "        if word not in nlp.Defaults.stop_words:\n",
    "            new_text.append(word)\n",
    "    return \" \".join(new_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1cd4064",
   "metadata": {},
   "outputs": [],
   "source": [
    "mystr=\"This is a sample text and we need to remove stopwords from it\"\n",
    "remove_stopwords_spacy(mystr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6ccb9f4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cbe336f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9842b38f",
   "metadata": {},
   "source": [
    "**Add a stop word to the existing list of spaCy:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d16c7b15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the word to the set of stop words. Use lowercase!\n",
    "nlp.Defaults.stop_words.add('aka')\n",
    "\n",
    "# Set the stop_word tag on the lexeme\n",
    "nlp.vocab['aka'].is_stop = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "778fdd3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp.vocab['aka'].is_stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d91b6366",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(nlp.Defaults.stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c28ebe3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b256d07",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "db5e5831",
   "metadata": {},
   "source": [
    "**To remove a stop word:** Alternatively, you may decide that `'always'` should not be considered a stop word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8f3c2cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp.vocab['aka'].is_stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c158d30e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the word from the set of stop words\n",
    "nlp.Defaults.stop_words.remove('aka')\n",
    "\n",
    "# Remove the stop_word tag from the lexeme\n",
    "nlp.vocab['aka'].is_stop = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d008124c",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp.vocab['aka'].is_stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07132ee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(nlp.Defaults.stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00f237ac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b220d2f2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c9706ce8",
   "metadata": {},
   "source": [
    "# 3. Text Pre-Processing on IMDB Dataset\n",
    "- Dataset: https://www.kaggle.com/datasets/lakshmi25npathi/imdb-dataset-of-50k-movie-reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea430cd4",
   "metadata": {},
   "source": [
    "## a. EDA on IMD Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a25f6fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"./datasets/imdb-dataset.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "211e2f32",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5a808df",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6872a0a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "465318b5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cafe7f3c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03759f1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the count of positive and negative reviews to ensure that the dataset is balanced\n",
    "df['sentiment'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d769aecf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe2914a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.catplot(x ='sentiment', kind='count', data = df);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "943778d4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36ccc133",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ab4db158",
   "metadata": {},
   "source": [
    "**Reduce the records from 50K to 1K for quick processing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fdd06e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save 1000 rows in a new dataframe\n",
    "temp_df = df.iloc[0:1000,:]\n",
    "temp_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff52d83e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check out the count of positive and negative reviews\n",
    "temp_df['sentiment'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c51f4022",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the dataframe to a new csv file\n",
    "temp_df.to_csv('datasets/imdb-dataset-1000.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15e0fc91",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b30712d8",
   "metadata": {},
   "source": [
    "## b. Case folding, removing digits, punctuations and substituting contractions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38b77f60",
   "metadata": {},
   "source": [
    "**Read the Dataset:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3491317d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"./datasets/imdb-dataset-1000.csv\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ed2a896",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.review[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2967df79",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "452efb1b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d78d918b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "import contractions\n",
    "from textblob import TextBlob\n",
    "\n",
    "def text_cleaning(mystr):\n",
    "    mystr = mystr.lower()     # case folding\n",
    "    mystr = re.sub('\\w*\\d\\w*', '', mystr) # removing digits\n",
    "    mystr = re.sub('\\n', ' ', mystr)      # replace new line characters with space\n",
    "    mystr = re.sub('[‚Äò‚Äô‚Äú‚Äù‚Ä¶]', '', mystr) # removing double quotes and single quotes\n",
    "    mystr = re.sub('<.*?>', '', mystr)   # removing html tags \n",
    "    mystr = re.sub('https?://\\S+|www.\\.\\S+', '', mystr) # removing URLs\n",
    "    mystr = ''.join([c for c in mystr if c not in string.punctuation])  # remove punctuations\n",
    "    mystr = ' '.join([contractions.fix(word) for word in mystr.split()]) # expand contractions\n",
    "    return mystr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa0137eb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3d7680d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['r_cleaned'] = df['review'].apply(lambda x : text_cleaning(x))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18f54b37",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8082fa37",
   "metadata": {},
   "source": [
    "## b. Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d05a58b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "df['r_tokenized'] = df['r_cleaned'].apply(lambda x: word_tokenize(x))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16419956",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cf756416",
   "metadata": {},
   "source": [
    "## c. Remove Stop Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "852940f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "stop_words = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "def remove_stopwords(tokenized_text):\n",
    "    new_words = [word for word in tokenized_text if word not in stop_words]\n",
    "    return new_words\n",
    "\n",
    "df['r_no_sw'] = df['r_tokenized'].apply(lambda token: remove_stopwords(token))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0707c22",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3ffd3adb",
   "metadata": {},
   "source": [
    "## d. Save the Pre-Processed Dataframe in a New CSV File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a49e3ba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# join the tokens of pre-processed text\n",
    "df['processed_reviews'] = df['r_no_sw'].apply(lambda x: ' '.join(x))\n",
    "\n",
    "new_df = pd.concat([df['sentiment'], df['processed_reviews']], axis=1)\n",
    "\n",
    "# save the resulting datafrrame to a new csv file\n",
    "new_df.to_csv('datasets/processed_imdb_reviews.csv', index=False)\n",
    "new_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ddf5787",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "441b30a2-6c21-4382-aefd-fc6ec541cbb0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62e5250a-1974-47f8-bce3-3af05b6dc36a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd5bbe20-83e9-4691-8eba-28bc69798fb6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "702a7d17-e290-43ab-93d6-7967b6328f21",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eaef413-a6ff-4f44-9255-46b36483428e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.display import HTML\n",
    "\n",
    "style = \"\"\"\n",
    "    <style>\n",
    "        body {\n",
    "            background-color: #f2fff2;\n",
    "        }\n",
    "        h1 {\n",
    "            text-align: center;\n",
    "            font-weight: bold;\n",
    "            font-size: 36px;\n",
    "            color: #4295F4;\n",
    "            text-decoration: underline;\n",
    "            padding-top: 15px;\n",
    "        }\n",
    "        \n",
    "        h2 {\n",
    "            text-align: left;\n",
    "            font-weight: bold;\n",
    "            font-size: 30px;\n",
    "            color: #4A000A;\n",
    "            text-decoration: underline;\n",
    "            padding-top: 10px;\n",
    "        }\n",
    "        \n",
    "        h3 {\n",
    "            text-align: left;\n",
    "            font-weight: bold;\n",
    "            font-size: 30px;\n",
    "            color: #f0081e;\n",
    "            text-decoration: underline;\n",
    "            padding-top: 5px;\n",
    "        }\n",
    "\n",
    "        \n",
    "        p {\n",
    "            text-align: center;\n",
    "            font-size: 12 px;\n",
    "            color: #0B9923;\n",
    "        }\n",
    "    </style>\n",
    "\"\"\"\n",
    "\n",
    "html_content = \"\"\"\n",
    "<h1>Hello</h1>\n",
    "<p>Hello World</p>\n",
    "<h2> Hello</h2>\n",
    "<h3> World </h3>\n",
    "\"\"\"\n",
    "\n",
    "HTML(style + html_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a2a1f78-cc5c-45b8-8555-ab23ac753b2e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
